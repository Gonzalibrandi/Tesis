{
  "data": {
    "edges": [
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-yVntr",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "question",
            "id": "Prompt-7vMII",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ChatInput-yVntr{œdataTypeœ:œChatInputœ,œidœ:œChatInput-yVntrœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-7vMII{œfieldNameœ:œquestionœ,œidœ:œPrompt-7vMIIœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatInput-yVntr",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-yVntrœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-7vMII",
        "targetHandle": "{œfieldNameœ:œquestionœ,œidœ:œPrompt-7vMIIœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-7vMII",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "OpenAIModel-4TVDv",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__Prompt-7vMII{œdataTypeœ:œPromptœ,œidœ:œPrompt-7vMIIœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-4TVDv{œfieldNameœ:œsystem_messageœ,œidœ:œOpenAIModel-4TVDvœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "Prompt-7vMII",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-7vMIIœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "OpenAIModel-4TVDv",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œOpenAIModel-4TVDvœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ContextTextMerger",
            "id": "CustomComponent-HweCC",
            "name": "unified_context",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "OpenAIModel-4TVDv",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__CustomComponent-HweCC{œdataTypeœ:œContextTextMergerœ,œidœ:œCustomComponent-HweCCœ,œnameœ:œunified_contextœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-4TVDv{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-4TVDvœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "CustomComponent-HweCC",
        "sourceHandle": "{œdataTypeœ:œContextTextMergerœ,œidœ:œCustomComponent-HweCCœ,œnameœ:œunified_contextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "OpenAIModel-4TVDv",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-4TVDvœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "OpenAIModel",
            "id": "OpenAIModel-4TVDv",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-KRmEh",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__OpenAIModel-4TVDv{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-4TVDvœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-KRmEh{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-KRmEhœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "OpenAIModel-4TVDv",
        "sourceHandle": "{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-4TVDvœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ChatOutput-KRmEh",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-KRmEhœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "JigsawStackAISearch",
            "id": "JigsawStackAISearch-OqFI6",
            "name": "search_results",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "input_data",
            "id": "CustomComponent-pE1Im",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__JigsawStackAISearch-OqFI6{œdataTypeœ:œJigsawStackAISearchœ,œidœ:œJigsawStackAISearch-OqFI6œ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}-CustomComponent-pE1Im{œfieldNameœ:œinput_dataœ,œidœ:œCustomComponent-pE1Imœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "JigsawStackAISearch-OqFI6",
        "sourceHandle": "{œdataTypeœ:œJigsawStackAISearchœ,œidœ:œJigsawStackAISearch-OqFI6œ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}",
        "target": "CustomComponent-pE1Im",
        "targetHandle": "{œfieldNameœ:œinput_dataœ,œidœ:œCustomComponent-pE1Imœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-yVntr",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "user_question",
            "id": "ConditionalRouter-92JKj",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ChatInput-yVntr{œdataTypeœ:œChatInputœ,œidœ:œChatInput-yVntrœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-ConditionalRouter-92JKj{œfieldNameœ:œuser_questionœ,œidœ:œConditionalRouter-92JKjœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatInput-yVntr",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-yVntrœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ConditionalRouter-92JKj",
        "targetHandle": "{œfieldNameœ:œuser_questionœ,œidœ:œConditionalRouter-92JKjœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "AdvancedTextExtractor",
            "id": "CustomComponent-N1Dmp",
            "name": "formatted_context",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "structured_data_text",
            "id": "CustomComponent-HweCC",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__CustomComponent-N1Dmp{œdataTypeœ:œAdvancedTextExtractorœ,œidœ:œCustomComponent-N1Dmpœ,œnameœ:œformatted_contextœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-HweCC{œfieldNameœ:œstructured_data_textœ,œidœ:œCustomComponent-HweCCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "CustomComponent-N1Dmp",
        "sourceHandle": "{œdataTypeœ:œAdvancedTextExtractorœ,œidœ:œCustomComponent-N1Dmpœ,œnameœ:œformatted_contextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-HweCC",
        "targetHandle": "{œfieldNameœ:œstructured_data_textœ,œidœ:œCustomComponent-HweCCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "NvidiaRerankComponent",
            "id": "NvidiaRerankComponent-DqD1n",
            "name": "reranked_documents",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "input_data",
            "id": "CustomComponent-N1Dmp",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__NvidiaRerankComponent-DqD1n{œdataTypeœ:œNvidiaRerankComponentœ,œidœ:œNvidiaRerankComponent-DqD1nœ,œnameœ:œreranked_documentsœ,œoutput_typesœ:[œDataœ]}-CustomComponent-N1Dmp{œfieldNameœ:œinput_dataœ,œidœ:œCustomComponent-N1Dmpœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "NvidiaRerankComponent-DqD1n",
        "sourceHandle": "{œdataTypeœ:œNvidiaRerankComponentœ,œidœ:œNvidiaRerankComponent-DqD1nœ,œnameœ:œreranked_documentsœ,œoutput_typesœ:[œDataœ]}",
        "target": "CustomComponent-N1Dmp",
        "targetHandle": "{œfieldNameœ:œinput_dataœ,œidœ:œCustomComponent-N1Dmpœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-yVntr",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "search_query",
            "id": "NvidiaRerankComponent-DqD1n",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ChatInput-yVntr{œdataTypeœ:œChatInputœ,œidœ:œChatInput-yVntrœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-NvidiaRerankComponent-DqD1n{œfieldNameœ:œsearch_queryœ,œidœ:œNvidiaRerankComponent-DqD1nœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatInput-yVntr",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-yVntrœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "NvidiaRerankComponent-DqD1n",
        "targetHandle": "{œfieldNameœ:œsearch_queryœ,œidœ:œNvidiaRerankComponent-DqD1nœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-yVntr",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "user_question",
            "id": "ConditionalRouter-RZ4gB",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ChatInput-yVntr{œdataTypeœ:œChatInputœ,œidœ:œChatInput-yVntrœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-ConditionalRouter-RZ4gB{œfieldNameœ:œuser_questionœ,œidœ:œConditionalRouter-RZ4gBœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatInput-yVntr",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-yVntrœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ConditionalRouter-RZ4gB",
        "targetHandle": "{œfieldNameœ:œuser_questionœ,œidœ:œConditionalRouter-RZ4gBœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ConditionalRouter",
            "id": "ConditionalRouter-92JKj",
            "name": "passthrough_result",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "search_query",
            "id": "Chroma-n7Ava",
            "inputTypes": [
              "Message"
            ],
            "type": "query"
          }
        },
        "id": "xy-edge__ConditionalRouter-92JKj{œdataTypeœ:œConditionalRouterœ,œidœ:œConditionalRouter-92JKjœ,œnameœ:œpassthrough_resultœ,œoutput_typesœ:[œMessageœ]}-Chroma-n7Ava{œfieldNameœ:œsearch_queryœ,œidœ:œChroma-n7Avaœ,œinputTypesœ:[œMessageœ],œtypeœ:œqueryœ}",
        "selected": false,
        "source": "ConditionalRouter-92JKj",
        "sourceHandle": "{œdataTypeœ:œConditionalRouterœ,œidœ:œConditionalRouter-92JKjœ,œnameœ:œpassthrough_resultœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Chroma-n7Ava",
        "targetHandle": "{œfieldNameœ:œsearch_queryœ,œidœ:œChroma-n7Avaœ,œinputTypesœ:[œMessageœ],œtypeœ:œqueryœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ConditionalRouter",
            "id": "ConditionalRouter-RZ4gB",
            "name": "passthrough_result",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "search_query",
            "id": "Chroma-TRdau",
            "inputTypes": [
              "Message"
            ],
            "type": "query"
          }
        },
        "id": "xy-edge__ConditionalRouter-RZ4gB{œdataTypeœ:œConditionalRouterœ,œidœ:œConditionalRouter-RZ4gBœ,œnameœ:œpassthrough_resultœ,œoutput_typesœ:[œMessageœ]}-Chroma-TRdau{œfieldNameœ:œsearch_queryœ,œidœ:œChroma-TRdauœ,œinputTypesœ:[œMessageœ],œtypeœ:œqueryœ}",
        "selected": false,
        "source": "ConditionalRouter-RZ4gB",
        "sourceHandle": "{œdataTypeœ:œConditionalRouterœ,œidœ:œConditionalRouter-RZ4gBœ,œnameœ:œpassthrough_resultœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Chroma-TRdau",
        "targetHandle": "{œfieldNameœ:œsearch_queryœ,œidœ:œChroma-TRdauœ,œinputTypesœ:[œMessageœ],œtypeœ:œqueryœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Chroma",
            "id": "Chroma-n7Ava",
            "name": "search_results",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "search_results",
            "id": "NvidiaRerankComponent-DqD1n",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__Chroma-n7Ava{œdataTypeœ:œChromaœ,œidœ:œChroma-n7Avaœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}-NvidiaRerankComponent-DqD1n{œfieldNameœ:œsearch_resultsœ,œidœ:œNvidiaRerankComponent-DqD1nœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "Chroma-n7Ava",
        "sourceHandle": "{œdataTypeœ:œChromaœ,œidœ:œChroma-n7Avaœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}",
        "target": "NvidiaRerankComponent-DqD1n",
        "targetHandle": "{œfieldNameœ:œsearch_resultsœ,œidœ:œNvidiaRerankComponent-DqD1nœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Chroma",
            "id": "Chroma-TRdau",
            "name": "search_results",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "search_results",
            "id": "NvidiaRerankComponent-DqD1n",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__Chroma-TRdau{œdataTypeœ:œChromaœ,œidœ:œChroma-TRdauœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}-NvidiaRerankComponent-DqD1n{œfieldNameœ:œsearch_resultsœ,œidœ:œNvidiaRerankComponent-DqD1nœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "Chroma-TRdau",
        "sourceHandle": "{œdataTypeœ:œChromaœ,œidœ:œChroma-TRdauœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}",
        "target": "NvidiaRerankComponent-DqD1n",
        "targetHandle": "{œfieldNameœ:œsearch_resultsœ,œidœ:œNvidiaRerankComponent-DqD1nœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-yVntr",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "query",
            "id": "JigsawStackAISearch-OqFI6",
            "inputTypes": [
              "Message"
            ],
            "type": "query"
          }
        },
        "id": "xy-edge__ChatInput-yVntr{œdataTypeœ:œChatInputœ,œidœ:œChatInput-yVntrœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-JigsawStackAISearch-OqFI6{œfieldNameœ:œqueryœ,œidœ:œJigsawStackAISearch-OqFI6œ,œinputTypesœ:[œMessageœ],œtypeœ:œqueryœ}",
        "selected": false,
        "source": "ChatInput-yVntr",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-yVntrœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "JigsawStackAISearch-OqFI6",
        "targetHandle": "{œfieldNameœ:œqueryœ,œidœ:œJigsawStackAISearch-OqFI6œ,œinputTypesœ:[œMessageœ],œtypeœ:œqueryœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "JigsawStackAIParser",
            "id": "CustomComponent-pE1Im",
            "name": "formatted_context_output",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "web_search_context",
            "id": "CustomComponent-HweCC",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__CustomComponent-pE1Im{œdataTypeœ:œJigsawStackAIParserœ,œidœ:œCustomComponent-pE1Imœ,œnameœ:œformatted_context_outputœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-HweCC{œfieldNameœ:œweb_search_contextœ,œidœ:œCustomComponent-HweCCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "CustomComponent-pE1Im",
        "sourceHandle": "{œdataTypeœ:œJigsawStackAIParserœ,œidœ:œCustomComponent-pE1Imœ,œnameœ:œformatted_context_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-HweCC",
        "targetHandle": "{œfieldNameœ:œweb_search_contextœ,œidœ:œCustomComponent-HweCCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "NVIDIAEmbeddingsComponent",
            "id": "NVIDIAEmbeddingsComponent-XGuHe",
            "name": "embeddings",
            "output_types": [
              "Embeddings"
            ]
          },
          "targetHandle": {
            "fieldName": "embedding",
            "id": "Chroma-TRdau",
            "inputTypes": [
              "Embeddings"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__NVIDIAEmbeddingsComponent-XGuHe{œdataTypeœ:œNVIDIAEmbeddingsComponentœ,œidœ:œNVIDIAEmbeddingsComponent-XGuHeœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-Chroma-TRdau{œfieldNameœ:œembeddingœ,œidœ:œChroma-TRdauœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "NVIDIAEmbeddingsComponent-XGuHe",
        "sourceHandle": "{œdataTypeœ:œNVIDIAEmbeddingsComponentœ,œidœ:œNVIDIAEmbeddingsComponent-XGuHeœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}",
        "target": "Chroma-TRdau",
        "targetHandle": "{œfieldNameœ:œembeddingœ,œidœ:œChroma-TRdauœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "NVIDIAEmbeddingsComponent",
            "id": "NVIDIAEmbeddingsComponent-XGuHe",
            "name": "embeddings",
            "output_types": [
              "Embeddings"
            ]
          },
          "targetHandle": {
            "fieldName": "embedding",
            "id": "Chroma-n7Ava",
            "inputTypes": [
              "Embeddings"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__NVIDIAEmbeddingsComponent-XGuHe{œdataTypeœ:œNVIDIAEmbeddingsComponentœ,œidœ:œNVIDIAEmbeddingsComponent-XGuHeœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-Chroma-n7Ava{œfieldNameœ:œembeddingœ,œidœ:œChroma-n7Avaœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "NVIDIAEmbeddingsComponent-XGuHe",
        "sourceHandle": "{œdataTypeœ:œNVIDIAEmbeddingsComponentœ,œidœ:œNVIDIAEmbeddingsComponent-XGuHeœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}",
        "target": "Chroma-n7Ava",
        "targetHandle": "{œfieldNameœ:œembeddingœ,œidœ:œChroma-n7Avaœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}"
      }
    ],
    "nodes": [
      {
        "data": {
          "id": "ChatInput-yVntr",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get chat inputs from the Playground.",
            "display_name": "Chat Input",
            "documentation": "https://docs.langflow.org/components-io#chat-input",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "files"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Chat Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.io import (\n    DropdownInput,\n    FileInput,\n    MessageTextInput,\n    MultilineInput,\n    Output,\n)\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_USER,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-input\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n    minimized = True\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Input Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n            input_types=[],\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n            temp_file=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Chat Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    async def message_response(self) -> Message:\n        # Ensure files is a list and filter out empty/None values\n        files = self.files if self.files else []\n        if files and not isinstance(files, list):\n            files = [files]\n        files = [f for f in files if f is not None and f != \"\"]\n\n        message = await Message.create(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=files,\n        )\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = await self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n"
              },
              "files": {
                "_input_type": "FileInput",
                "advanced": true,
                "display_name": "Files",
                "dynamic": false,
                "fileTypes": [
                  "csv",
                  "json",
                  "pdf",
                  "txt",
                  "md",
                  "mdx",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "file_path": "",
                "info": "Files to be sent with the message.",
                "list": true,
                "list_add_label": "Add More",
                "name": "files",
                "placeholder": "",
                "required": false,
                "show": true,
                "temp_file": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "file",
                "value": ""
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Input Text",
                "dynamic": false,
                "info": "Message to be passed as input.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "external_options": {},
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "ChatInput"
        },
        "dragging": false,
        "id": "ChatInput-yVntr",
        "measured": {
          "height": 202,
          "width": 320
        },
        "position": {
          "x": 108.1162742296263,
          "y": -161.31865586763652
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "CustomComponent-N1Dmp",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Extracts text and metadata from documents, compatible with both retrievers and rerankers.",
            "display_name": "Advanced Text Extractor",
            "documentation": "",
            "edited": true,
            "field_order": [
              "input_data",
              "sep"
            ],
            "frozen": false,
            "icon": "file-search",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Formatted Context",
                "group_outputs": false,
                "hidden": null,
                "method": "extract_and_format",
                "name": "formatted_context",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import List\r\nfrom langflow.custom import Component\r\nfrom langflow.io import HandleInput, MessageTextInput, Output\r\nfrom langflow.schema import Data, Message\r\n\r\nclass AdvancedTextExtractor(Component):\r\n    name = \"AdvancedTextExtractor\"\r\n    display_name = \"Advanced Text Extractor\"\r\n    description = \"Extracts text and metadata from documents, compatible with both retrievers and rerankers.\"\r\n    icon = \"file-search\"\r\n\r\n    inputs = [\r\n        HandleInput(\r\n            name=\"input_data\",\r\n            display_name=\"Input Data List\",\r\n            info=\"A list of Data objects from a retriever or reranker.\",\r\n            is_list=True,\r\n            input_types=[\"Data\"],\r\n            required=True,\r\n        ),\r\n        MessageTextInput(\r\n            name=\"sep\",\r\n            display_name=\"Separator\",\r\n            advanced=True,\r\n            value=\"\\n\\n---\\n\\n\",\r\n            info=\"String used to separate each formatted text chunk.\",\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(\r\n            display_name=\"Formatted Context\",\r\n            name=\"formatted_context\",\r\n            info=\"A single block of text with all extracted and formatted content.\",\r\n            method=\"extract_and_format\"\r\n        ),\r\n    ]\r\n\r\n    def extract_and_format(self) -> Message:\r\n        \"\"\"\r\n        Iterates through Data objects, intelligently extracts metadata from either\r\n        the top-level .metadata or the nested .data attribute, and formats the output.\r\n        \"\"\"\r\n        documents = self.input_data or []\r\n        \r\n        if not documents:\r\n            self.status = \"No documents received.\"\r\n            return Message(text=\"\")\r\n\r\n        formatted_chunks = []\r\n        for i, doc in enumerate(documents):\r\n            if isinstance(doc, Data):\r\n                text_content = doc.text or \"\"\r\n                \r\n                # --- LÓGICA INTELIGENTE DE EXTRACCIÓN ---\r\n                # 1. Empezamos con el metadata de nivel superior (salida de FAISS).\r\n                combined_info = getattr(doc, 'metadata', None) or {}\r\n                \r\n                # 2. Si existe el atributo .data (salida del Reranker),\r\n                #    actualizamos nuestro diccionario, priorizando esta información.\r\n                if hasattr(doc, 'data') and doc.data:\r\n                    combined_info.update(doc.data)\r\n\r\n                # 3. Ahora extraemos la información del diccionario combinado.\r\n                origin = combined_info.get(\"origin\", \"Unknown Origin\")\r\n                source = combined_info.get(\"source\", \"Unknown Source\")\r\n                score = combined_info.get(\"relevance_score\")\r\n\r\n                # Construimos el encabezado para cada chunk\r\n                header = f\"--- Context Chunk {i + 1} (From: {origin}) ---\\n\"\r\n                header += f\"Source File: {source}\\n\"\r\n                if score is not None:\r\n                    header += f\"Relevance Score: {score:.4f}\\n\"\r\n                \r\n                chunk_block = f\"{header}\\n{text_content}\"\r\n                formatted_chunks.append(chunk_block)\r\n        \r\n        combined_text = self.sep.join(formatted_chunks)\r\n        \r\n        self.status = f\"Successfully formatted {len(formatted_chunks)} text chunks.\"\r\n        return Message(text=combined_text)"
              },
              "input_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Input Data List",
                "dynamic": false,
                "info": "A list of Data objects from a retriever or reranker.",
                "input_types": [
                  "Data"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "input_data",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "sep": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Separator",
                "dynamic": false,
                "info": "String used to separate each formatted text chunk.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sep",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "\n"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "AdvancedTextExtractor"
        },
        "dragging": false,
        "id": "CustomComponent-N1Dmp",
        "measured": {
          "height": 180,
          "width": 320
        },
        "position": {
          "x": 2251.878130068232,
          "y": 61.04625803318363
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "OpenAIModel-4TVDv",
          "node": {
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "beta": false,
            "category": "openai",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generates text using OpenAI LLMs.",
            "display_name": "OpenAI",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_tokens",
              "model_kwargs",
              "json_mode",
              "model_name",
              "openai_api_base",
              "api_key",
              "temperature",
              "seed",
              "max_retries",
              "timeout"
            ],
            "frozen": false,
            "icon": "OpenAI",
            "key": "OpenAIModel",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {
              "keywords": [
                "model",
                "llm",
                "language model",
                "large language model"
              ]
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "method": "text_response",
                "name": "text_output",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Language Model",
                "group_outputs": false,
                "method": "build_model",
                "name": "model_output",
                "tool_mode": true,
                "types": [
                  "LanguageModel"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 0.000001,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "OpenAI API Key",
                "dynamic": false,
                "info": "The OpenAI API Key to use for the OpenAI model.",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import (\n    OPENAI_CHAT_MODEL_NAMES,\n    OPENAI_REASONING_MODEL_NAMES,\n)\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs.inputs import BoolInput, DictInput, DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput\nfrom langflow.logging import logger\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_CHAT_MODEL_NAMES + OPENAI_REASONING_MODEL_NAMES,\n            value=OPENAI_CHAT_MODEL_NAMES[0],\n            combobox=True,\n            real_time_refresh=True,\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n            required=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n            show=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        IntInput(\n            name=\"max_retries\",\n            display_name=\"Max Retries\",\n            info=\"The maximum number of retries to make when generating.\",\n            advanced=True,\n            value=5,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"The timeout for requests to OpenAI completion API.\",\n            advanced=True,\n            value=700,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        logger.debug(f\"Executing request with model: {self.model_name}\")\n        parameters = {\n            \"api_key\": SecretStr(self.api_key).get_secret_value() if self.api_key else None,\n            \"model_name\": self.model_name,\n            \"max_tokens\": self.max_tokens or None,\n            \"model_kwargs\": self.model_kwargs or {},\n            \"base_url\": self.openai_api_base or \"https://api.openai.com/v1\",\n            \"max_retries\": self.max_retries,\n            \"timeout\": self.timeout,\n        }\n\n        # TODO: Revisit if/once parameters are supported for reasoning models\n        unsupported_params_for_reasoning_models = [\"temperature\", \"seed\"]\n\n        if self.model_name not in OPENAI_REASONING_MODEL_NAMES:\n            parameters[\"temperature\"] = self.temperature if self.temperature is not None else 0.1\n            parameters[\"seed\"] = self.seed\n        else:\n            params_str = \", \".join(unsupported_params_for_reasoning_models)\n            logger.debug(f\"{self.model_name} is a reasoning model, {params_str} are not configurable. Ignoring.\")\n\n        output = ChatOpenAI(**parameters)\n        if self.json_mode:\n            output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None) -> dict:\n        if field_name in {\"base_url\", \"model_name\", \"api_key\"} and field_value in OPENAI_REASONING_MODEL_NAMES:\n            build_config[\"temperature\"][\"show\"] = False\n            build_config[\"seed\"][\"show\"] = False\n            # Hide system_message for o1 models - currently unsupported\n            if field_value.startswith(\"o1\") and \"system_message\" in build_config:\n                build_config[\"system_message\"][\"show\"] = False\n        if field_name in {\"base_url\", \"model_name\", \"api_key\"} and field_value in OPENAI_CHAT_MODEL_NAMES:\n            build_config[\"temperature\"][\"show\"] = True\n            build_config[\"seed\"][\"show\"] = True\n            if \"system_message\" in build_config:\n                build_config[\"system_message\"][\"show\"] = True\n        return build_config\n"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "json_mode": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "JSON Mode",
                "dynamic": false,
                "info": "If True, it will output JSON regardless of passing a schema.",
                "list": false,
                "list_add_label": "Add More",
                "name": "json_mode",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "max_retries": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Retries",
                "dynamic": false,
                "info": "The maximum number of retries to make when generating.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_retries",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 5
              },
              "max_tokens": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Tokens",
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_tokens",
                "placeholder": "",
                "range_spec": {
                  "max": 128000,
                  "min": 0,
                  "step": 0.1,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": ""
              },
              "model_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Model Kwargs",
                "dynamic": false,
                "info": "Additional keyword arguments to pass to the model.",
                "list": false,
                "list_add_label": "Add More",
                "name": "model_kwargs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": true,
                "dialog_inputs": {},
                "display_name": "Model Name",
                "dynamic": false,
                "info": "",
                "name": "model_name",
                "options": [
                  "gpt-4o-mini",
                  "gpt-4o",
                  "gpt-4.1",
                  "gpt-4.1-mini",
                  "gpt-4.1-nano",
                  "gpt-4.5-preview",
                  "gpt-4-turbo",
                  "gpt-4-turbo-preview",
                  "gpt-4",
                  "gpt-3.5-turbo",
                  "o1",
                  "o1-mini",
                  "o1-pro",
                  "o3-mini",
                  "o3",
                  "o3-pro",
                  "o4-mini",
                  "o4-mini-high"
                ],
                "options_metadata": [],
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "gpt-4o-mini"
              },
              "openai_api_base": {
                "_input_type": "StrInput",
                "advanced": true,
                "display_name": "OpenAI API Base",
                "dynamic": false,
                "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "openai_api_base",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "seed": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Seed",
                "dynamic": false,
                "info": "The seed controls the reproducibility of the job.",
                "list": false,
                "list_add_label": "Add More",
                "name": "seed",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "System Message",
                "dynamic": false,
                "info": "System message to pass to the model.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "SliderInput",
                "advanced": false,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "",
                "max_label": "",
                "max_label_icon": "",
                "min_label": "",
                "min_label_icon": "",
                "name": "temperature",
                "placeholder": "",
                "range_spec": {
                  "max": 1,
                  "min": 0,
                  "step": 0.01,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "title_case": false,
                "tool_mode": false,
                "type": "slider",
                "value": 0
              },
              "timeout": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Timeout",
                "dynamic": false,
                "info": "The timeout for requests to OpenAI completion API.",
                "list": false,
                "list_add_label": "Add More",
                "name": "timeout",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 700
              }
            },
            "tool_mode": false
          },
          "selected_output": "text_output",
          "showNode": true,
          "type": "OpenAIModel"
        },
        "dragging": false,
        "id": "OpenAIModel-4TVDv",
        "measured": {
          "height": 535,
          "width": 320
        },
        "position": {
          "x": 3098.273873968896,
          "y": 42.182344062003516
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-7vMII",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {
              "template": [
                "question"
              ]
            },
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "documentation": "",
            "edited": false,
            "error": null,
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "frozen": false,
            "full_path": null,
            "icon": "braces",
            "is_composition": null,
            "is_input": null,
            "is_output": null,
            "legacy": false,
            "metadata": {},
            "minimized": false,
            "name": "",
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Prompt",
                "group_outputs": false,
                "hidden": null,
                "method": "build_prompt",
                "name": "prompt",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "priority": null,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom.custom_component.component import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"braces\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "question": {
                "advanced": false,
                "display_name": "question",
                "dynamic": false,
                "field_type": "str",
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "question",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "template": {
                "_input_type": "PromptInput",
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "You are a high-performance assistant specialized in context analysis. \nYour sole task is to **answer the question** based strictly on the provided context (Structured Context and Web Context).\n\n**PRIORITIZATION RULE (CRITICAL):**\n1.  **Prioritize Sources:** When synthesizing the answer, you **must prioritize** information found in **PDF** and **GitHub** content.\n2.  **Web as Supplemental:** Use the **Web** context only to fill gaps, provide supporting details, or complete the answer if the **PDF** and **GitHub** sources are insufficient or absent. If the primary sources are comprehensive, the Web's contribution should be minimal and reflect a supporting role.\n3.  **LLM as Fallback:** Only use **LLM prior knowledge** if the information is entirely missing or insufficient across all provided contexts (PDF, GitHub, and Web).\n\n**STEP-BY-STEP INSTRUCTIONS:**\n1. Identify the core information and formulate the main part of the answer using only the **PDF and GitHub** context.\n2. Review the **Web context** to find any details that **clarify, support, or enrich** the core answer you formulated.\n3. Construct the final, complete answer by integrating the supplemental details from the web into the core answer from your primary sources.\n4. Calculate the Source Traceability percentages based on the contribution of each source to the final answer, strictly adhering to the hierarchy.\n\nYou must **strictly adhere** to the following output format. **DO NOT** include any other explanation or text outside of this format.\n\n1. Always begin with the literal header **Answer:**. \n2. Provide the answer to the question immediately after the **Answer:** header. The response must be concise, complete, and high-quality. \n3. After the answer, skip one line and add the literal header **Source Traceability:**. \n4. Under **Source Traceability:**, list the sources you used. You may only use the source types: **PDF**, **GitHub**, **Web**, or **LLM prior knowledge**. \n5. You must assign a percentage contribution, summing to a total of 100% across all used sources. The percentage assigned must reflect the **prioritization rule** (PDF/GitHub should receive higher percentages when used). \n6. Format each source as a Markdown list item (-) followed by the type and the percentage: - [Type]: [Percentage]%. \n7. **Suggested Questions Policy:** \n    7.1. Only include a **Suggested Questions** section if BOTH of the following conditions are met: \n        - The original question was appropriate for a RAG system (i.e., it seeks factual, contextual, or document-based information). \n        - The question is directly related to information from **PDFs or GitHub repositories**. \n    7.2. In this section, list 1–3 related or alternative questions that could yield better or more specific results. \n    7.3. If the question does not meet these criteria, **omit this section entirely**.\n8. **Critical Rule for Missing Information:** \n    8.1. If no relevant information is found in the provided context, or you cannot confidently answer, you MUST still fill the **Answer** section using your prior knowledge. \n    8.2. Leaving the **Answer** section empty is strictly forbidden. \n\n**STRICT REQUIRED OUTPUT FORMAT:**\n**Answer:** [Your concise answer based on the context. If none, respond using your prior knowledge.]\n**Source Traceability:** \n- PDF: [Percentage]% \n- GitHub: [Percentage]% \n- Web: [Percentage]% \n- LLM prior knowledge: [Percentage]%\n**Suggested Questions:** \n- [Question 1] \n- [Question 2] \n- [Question 3] \n\n(Only include this section if the question was appropriate for RAG and related to PDFs or GitHub repositories.)\n\nGiven the context, answer the question as best as possible.\n\nQuestion: {question} \n\nAnswer:"
              },
              "tool_placeholder": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Tool Placeholder",
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "tool_placeholder",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "prompt",
          "type": "Prompt"
        },
        "dragging": false,
        "id": "Prompt-7vMII",
        "measured": {
          "height": 364,
          "width": 320
        },
        "position": {
          "x": 2692.9998279050187,
          "y": 301.00999820702043
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "CustomComponent-pE1Im",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Extracts and formats the AI Overview and search results from the JigsawStack AI Search component into a single, structured text.",
            "display_name": "JigsawStack AI Search Parser",
            "documentation": "",
            "edited": true,
            "field_order": [
              "input_data",
              "separator"
            ],
            "frozen": false,
            "icon": "puzzle-piece",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Formatted Search Context",
                "group_outputs": false,
                "hidden": null,
                "method": "parse_search_results",
                "name": "formatted_context_output",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import pandas as pd\r\nfrom typing import Any, List, Dict\r\n\r\nfrom langflow.custom.custom_component.component import Component\r\nfrom langflow.io import (\r\n    DataInput,\r\n    Output,\r\n    MessageTextInput,\r\n)\r\nfrom langflow.schema import Data\r\nfrom langflow.schema.message import Message\r\n\r\nclass JigsawStackAIParserComponent(Component):\r\n    display_name = \"JigsawStack AI Search Parser\"\r\n    description = \"Extracts and formats the AI Overview and search results from the JigsawStack AI Search component into a single, structured text.\"\r\n    icon = \"puzzle-piece\"\r\n    name = \"JigsawStackAIParser\"\r\n\r\n    # ------------------ Inputs ------------------\r\n    inputs = [\r\n        DataInput(\r\n            name=\"input_data\",\r\n            display_name=\"Search Results Data\",\r\n            input_types=[\"Data\"],\r\n            info=\"The output Data object from the JigsawStack AI Search component.\",\r\n            required=True,\r\n            is_list=False, # Esperamos UN solo objeto Data\r\n        ),\r\n        MessageTextInput(\r\n            name=\"separator\",\r\n            display_name=\"Result Separator\",\r\n            advanced=True,\r\n            value=\"\\n---\\n\",\r\n            info=\"String used to separate the content of each individual search result block.\",\r\n        ),\r\n    ]\r\n\r\n    # ------------------ Outputs ------------------\r\n    outputs = [\r\n        Output(\r\n            display_name=\"Formatted Search Context\",\r\n            name=\"formatted_context_output\",\r\n            method=\"parse_search_results\",\r\n        ),\r\n    ]\r\n\r\n    # ------------------ Helpers ------------------\r\n    def _format_result(self, result: Dict[str, Any], index: int) -> str:\r\n        \"\"\"Formatea un resultado individual de la búsqueda.\"\"\"\r\n        title = result.get(\"title\", \"No Title\")\r\n        url = result.get(\"url\", \"No URL\")\r\n        \r\n        # Limitamos los snippets para mantener la coherencia y evitar texto excesivo\r\n        snippets = \"\\n\".join([f\"- {s}\" for s in result.get(\"snippets\", [])[:3]]) # Limitamos a 3 snippets por resultado\r\n        \r\n        return (\r\n            f\"Result {index + 1}: {title}\\n\"\r\n            f\"Source: {url}\\n\"\r\n            f\"Relevant Snippets:\\n\"\r\n            f\"{snippets}\\n\"\r\n        )\r\n        \r\n    def _extract_search_data(self, input_item: Any) -> Dict[str, Any]:\r\n        \"\"\"Extrae el diccionario de resultados del objeto Data de Langflow.\"\"\"\r\n        if isinstance(input_item, Data):\r\n            # Asumimos que el valor del objeto Data es el diccionario de la API\r\n            return input_item.data if isinstance(input_item.data, dict) else {}\r\n        if isinstance(input_item, dict):\r\n            return input_item\r\n        return {}\r\n\r\n\r\n    # ------------------ Output Method ------------------\r\n    def parse_search_results(self) -> Message:\r\n        \"\"\"\r\n        Extrae el AI Overview y los resultados de la búsqueda, los formatea\r\n        y los combina en un único mensaje estructurado.\r\n        \"\"\"\r\n        \r\n        # 1. Extraer el diccionario de resultados de la API\r\n        search_data = self._extract_search_data(self.input_data)\r\n        \r\n        if not search_data:\r\n            self.status = \"Error: Input data is empty or not in the expected format (JigsawStack Data).\"\r\n            return Message(text=\"\")\r\n\r\n        # 2. Extraer AI Overview y Results\r\n        ai_overview = search_data.get(\"ai_overview\", \"\")\r\n        # NOTA: La lista 'results' ya viene limitada desde el nodo JigsawStackAIWebSearchComponent.\r\n        results = search_data.get(\"results\", [])\r\n        separator = self.separator \r\n\r\n        # 3. Formatear los resultados individuales (links/snippets)\r\n        formatted_results = []\r\n        for i, result in enumerate(results):\r\n            formatted_results.append(self._format_result(result, i))\r\n\r\n        # 4. Combinar la lista de resultados formateados en un bloque\r\n        supporting_results_block = separator.join(formatted_results)\r\n\r\n        # 5. Construir la lista de contenido final\r\n        combined_text_parts = []\r\n\r\n        if ai_overview:\r\n            combined_text_parts.append(\"## AI Overview\\n\" + ai_overview)\r\n            \r\n        if supporting_results_block:\r\n            # Añadimos un separador solo si hay AI Overview y resultados\r\n            if ai_overview:\r\n                 combined_text_parts.append(separator)\r\n\r\n            combined_text_parts.append(\"## Supporting Search Results\")\r\n            combined_text_parts.append(supporting_results_block)\r\n\r\n        # 6. Unir todas las partes principales\r\n        final_text = \"\\n\".join(combined_text_parts)\r\n        \r\n        self.status = f\"Parsed {len(results)} search results and AI Overview. Total length: {len(final_text)}.\"\r\n        \r\n        # 7. Devolver como un objeto Message de Langflow\r\n        return Message(text=final_text)"
              },
              "input_data": {
                "_input_type": "DataInput",
                "advanced": false,
                "display_name": "Search Results Data",
                "dynamic": false,
                "info": "The output Data object from the JigsawStack AI Search component.",
                "input_types": [
                  "Data"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_data",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "separator": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Result Separator",
                "dynamic": false,
                "info": "String used to separate the content of each individual search result block.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "separator",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "\n---\n"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "JigsawStackAIParser"
        },
        "dragging": false,
        "id": "CustomComponent-pE1Im",
        "measured": {
          "height": 196,
          "width": 320
        },
        "position": {
          "x": 1152.7283539333644,
          "y": -383.3815187683541
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "CustomComponent-HweCC",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Combines two separate text inputs (e.g., Structured Data and Web Search) into one final context string.",
            "display_name": "Context Text Merger",
            "documentation": "",
            "edited": true,
            "field_order": [
              "web_search_context",
              "structured_data_text"
            ],
            "frozen": false,
            "icon": "code-pull-request-draft",
            "legacy": false,
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Final Unified Context",
                "group_outputs": false,
                "hidden": null,
                "method": "merge_context",
                "name": "unified_context",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.custom.custom_component.component import Component\nfrom langflow.io import (\n    MessageTextInput,\n    Output,\n    MessageInput,\n)\nfrom langflow.schema.message import Message\nfrom typing import Any\n\nclass ContextTextMerger(Component):\n    display_name = \"Context Text Merger\"\n    description = \"Combines two separate text inputs (e.g., Structured Data and Web Search) into one final context string.\"\n    icon = \"code-pull-request-draft\"\n    name = \"ContextTextMerger\"\n\n    # ------------------ Inputs ------------------\n    inputs = [\n        MessageInput(\n            name=\"web_search_context\",\n            display_name=\"Web Search Context (from Combine Documents)\",\n            info=\"The combined text from the Web Search/Combine Documents component.\",\n            required=False,\n        ),\n        MessageInput(\n            name=\"structured_data_text\",\n            display_name=\"Structured Data Text (from Parser)\",\n            info=\"The formatted text from the DataFrame/Parser component.\",\n            required=True,\n        ),\n    ]\n\n    # ------------------ Outputs ------------------\n    outputs = [\n        Output(\n            display_name=\"Final Unified Context\",\n            name=\"unified_context\",\n            method=\"merge_context\",\n        ),\n    ]\n\n    # ------------------ Helper ------------------\n    def _to_str(self, v: Any) -> str:\n        \"\"\"Extrae la cadena de texto de un objeto Message o valor simple.\"\"\"\n        if v is None:\n            return \"\"\n        if isinstance(v, Message):\n            return v.text if v.text else \"\"\n        return str(v)\n\n    # ------------------ Output Method ------------------\n    def merge_context(self) -> Message:\n        \"\"\"Combina los dos contextos con encabezados claros.\"\"\"\n        \n        # 1. Extraer el texto de las entradas\n        data_text = self._to_str(self.structured_data_text).strip()\n        web_text = self._to_str(self.web_search_context).strip()\n        \n        unified_context_parts = []\n        \n        # 2. Formatear y añadir el contexto estructurado\n        if data_text:\n            unified_context_parts.append(\n                \"--- INICIO DATOS ESTRUCTURADOS ---\\n\"\n                f\"{data_text}\\n\"\n                \"--- FIN DATOS ESTRUCTURADOS ---\"\n            )\n\n        # 3. Formatear y añadir el contexto de búsqueda web\n        if web_text:\n            # Añadir un separador si ya hay datos estructurados\n            if unified_context_parts:\n                 unified_context_parts.append(\"\\n\\n\") \n                 \n            unified_context_parts.append(\n                \"--- INICIO CONTEXTO WEB ---\\n\"\n                f\"{web_text}\\n\"\n                \"--- FIN CONTEXTO WEB ---\"\n            )\n\n        # 4. Unir todas las partes\n        final_context = \"\".join(unified_context_parts)\n        \n        if not final_context:\n            self.status = \"No context was provided to merge.\"\n            return Message(text=\"\")\n\n        self.status = f\"Successfully merged two contexts into a single string of length {len(final_context)}.\"\n        \n        # 5. Devolver como un único objeto Message\n        return Message(text=final_context)\n"
              },
              "structured_data_text": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Structured Data Text (from Parser)",
                "dynamic": false,
                "info": "The formatted text from the DataFrame/Parser component.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "structured_data_text",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "web_search_context": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Web Search Context (from Combine Documents)",
                "dynamic": false,
                "info": "The combined text from the Web Search/Combine Documents component.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "web_search_context",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "ContextTextMerger"
        },
        "dragging": false,
        "id": "CustomComponent-HweCC",
        "measured": {
          "height": 316,
          "width": 320
        },
        "position": {
          "x": 2676.2925591946964,
          "y": -251.64535435264182
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ConditionalRouter-RZ4gB",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Routes input messages based on a conditional check.",
            "display_name": "Conditional Router",
            "documentation": "",
            "edited": true,
            "field_order": [
              "input_text",
              "operator",
              "match_text",
              "case_sensitive",
              "true_case_message",
              "false_case_message",
              "user_question",
              "max_iterations",
              "default_route"
            ],
            "frozen": false,
            "icon": "split",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Passthrough (if True)",
                "group_outputs": false,
                "hidden": null,
                "method": "text_response",
                "name": "passthrough_result",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "case_sensitive": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Case Sensitive",
                "dynamic": false,
                "info": "If true, the comparison will be case sensitive.",
                "list": false,
                "list_add_label": "Add More",
                "name": "case_sensitive",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import re\r\nfrom typing import Any, List\r\n\r\nfrom langflow.custom.custom_component.component import Component\r\nfrom langflow.io import (\r\n    BoolInput,\r\n    DropdownInput,\r\n    IntInput,\r\n    MessageInput,\r\n    MessageTextInput,\r\n    Output,\r\n)\r\nfrom langflow.schema.message import Message\r\n\r\n\r\nclass ConditionalRouterComponent(Component):\r\n    display_name = \"Conditional Router\"\r\n    description = \"Routes input messages based on a conditional check.\"\r\n    icon = \"split\"\r\n    name = \"ConditionalRouter\"\r\n\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.__iteration_updated = False\r\n\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"input_text\",\r\n            display_name=\"Text Input\",\r\n            info=\"The primary text input for the operation.\",\r\n            required=True,\r\n        ),\r\n        DropdownInput(\r\n            name=\"operator\",\r\n            display_name=\"Operator\",\r\n            options=[\r\n                \"equals\", \"not equals\", \"contains\", \"starts with\", \"ends with\", \"regex\",\r\n                \"less than\", \"less than or equal\", \"greater than\", \"greater than or equal\",\r\n            ],\r\n            info=\"The operator to apply for comparing the texts.\",\r\n            value=\"equals\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"match_text\",\r\n            display_name=\"Match Text\",\r\n            info=\"The text input to compare against.\",\r\n            required=True,\r\n        ),\r\n        BoolInput(\r\n            name=\"case_sensitive\",\r\n            display_name=\"Case Sensitive\",\r\n            info=\"If true, the comparison will be case sensitive.\",\r\n            value=True,\r\n            advanced=True,\r\n        ),\r\n        MessageInput(\r\n            name=\"true_case_message\",\r\n            display_name=\"True Case Message\",\r\n            info=\"The message to pass if the condition is True.\",\r\n            advanced=True,\r\n        ),\r\n        MessageInput(\r\n            name=\"false_case_message\",\r\n            display_name=\"False Case Message\",\r\n            info=\"The message to pass if the condition is False.\",\r\n            advanced=True,\r\n        ),\r\n        MessageInput(\r\n            name=\"user_question\",\r\n            display_name=\"Passthrough Message\",\r\n            info=\"The message to pass through if the condition is True.\",\r\n        ),\r\n        IntInput(\r\n            name=\"max_iterations\",\r\n            display_name=\"Max Iterations\",\r\n            info=\"The maximum number of iterations for the conditional router.\",\r\n            value=10,\r\n            advanced=True,\r\n        ),\r\n        DropdownInput(\r\n            name=\"default_route\",\r\n            display_name=\"Default Route\",\r\n            options=[\"true_result\", \"false_result\"],\r\n            info=\"The default route to take when max iterations are reached.\",\r\n            value=\"false_result\",\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Passthrough (if True)\", name=\"passthrough_result\", method=\"text_response\"),\r\n    ]\r\n\r\n    def _pre_run_setup(self):\r\n        self.__iteration_updated = False\r\n\r\n    def _to_str(self, v: Any) -> str:\r\n        if v is None:\r\n            return \"\"\r\n        if hasattr(v, \"content\"):\r\n            return str(v.content or \"\")\r\n        return str(v)\r\n\r\n    def evaluate_condition(self) -> bool:\r\n        input_s = self._to_str(self.input_text)\r\n        match_s = self._to_str(self.match_text)\r\n        operator = self.operator\r\n        case_sensitive = self.case_sensitive\r\n\r\n        if not case_sensitive and operator != \"regex\":\r\n            input_s = input_s.lower()\r\n            match_s = match_s.lower()\r\n\r\n        try:\r\n            if operator == \"equals\": return input_s == match_s\r\n            if operator == \"not equals\": return input_s != match_s\r\n            if operator == \"contains\": return match_s in input_s\r\n            if operator == \"starts with\": return input_s.startswith(match_s)\r\n            if operator == \"ends with\": return input_s.endswith(match_s)\r\n            if operator == \"regex\": return bool(re.search(match_s, input_s))\r\n            \r\n            if operator in [\"less than\", \"less than or equal\", \"greater than\", \"greater than or equal\"]:\r\n                input_num = float(input_s)\r\n                match_num = float(match_s)\r\n                if operator == \"less than\": return input_num < match_num\r\n                if operator == \"less than or equal\": return input_num <= match_num\r\n                if operator == \"greater than\": return input_num > match_num\r\n                if operator == \"greater than or equal\": return input_num >= match_num\r\n        except (ValueError, TypeError, re.error):\r\n            return False\r\n        return False\r\n\r\n    def iterate_and_stop_once(self, route_to_stop: str):\r\n        if not self.__iteration_updated:\r\n            self.update_ctx({f\"{self._id}_iteration\": self.ctx.get(f\"{self._id}_iteration\", 0) + 1})\r\n            self.__iteration_updated = True\r\n            if self.ctx.get(f\"{self._id}_iteration\", 0) >= self.max_iterations and route_to_stop == self.default_route:\r\n                route_to_stop = \"true_result\" if route_to_stop == \"false_result\" else \"false_result\"\r\n            self.stop(route_to_stop)\r\n\r\n    def true_response(self) -> Message:\r\n        if self.evaluate_condition():\r\n            self.status = self.true_case_message\r\n            self.iterate_and_stop_once(\"false_result\")\r\n            return self.true_case_message\r\n        self.iterate_and_stop_once(\"true_result\")\r\n        return Message(content=\"\")\r\n\r\n    def false_response(self) -> Message:\r\n        if not self.evaluate_condition():\r\n            self.status = self.false_case_message\r\n            self.iterate_and_stop_once(\"true_result\")\r\n            return self.false_case_message\r\n        self.iterate_and_stop_once(\"false_result\")\r\n        return Message(content=\"\")\r\n\r\n    def text_response(self) -> Message: # Ya no es Optional[Message]\r\n        if self.evaluate_condition():\r\n            self.status = \"Condition is True -> Passing message through.\"\r\n            return self.user_question\r\n    \r\n        # Si la condición es False, devuelve un mensaje VACÍO\r\n        self.status = \"Condition is False -> Passing an empty message.\"\r\n        return Message(content=\"\") # <-- Solución con texto vacío"
              },
              "default_route": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Default Route",
                "dynamic": false,
                "info": "The default route to take when max iterations are reached.",
                "name": "default_route",
                "options": [
                  "true_result",
                  "false_result"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "false_result"
              },
              "false_case_message": {
                "_input_type": "MessageInput",
                "advanced": true,
                "display_name": "False Case Message",
                "dynamic": false,
                "info": "The message to pass if the condition is False.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "false_case_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "input_text": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Text Input",
                "dynamic": false,
                "info": "The primary text input for the operation.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_text",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "match_text": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Match Text",
                "dynamic": false,
                "info": "The text input to compare against.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "match_text",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "PDF_INGESTED"
              },
              "max_iterations": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Iterations",
                "dynamic": false,
                "info": "The maximum number of iterations for the conditional router.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_iterations",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 10
              },
              "operator": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Operator",
                "dynamic": false,
                "info": "The operator to apply for comparing the texts.",
                "name": "operator",
                "options": [
                  "equals",
                  "not equals",
                  "contains",
                  "starts with",
                  "ends with",
                  "regex",
                  "less than",
                  "less than or equal",
                  "greater than",
                  "greater than or equal"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "equals"
              },
              "true_case_message": {
                "_input_type": "MessageInput",
                "advanced": true,
                "display_name": "True Case Message",
                "dynamic": false,
                "info": "The message to pass if the condition is True.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "true_case_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "user_question": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Passthrough Message",
                "dynamic": false,
                "info": "The message to pass through if the condition is True.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "user_question",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "true_result",
          "showNode": true,
          "type": "ConditionalRouter"
        },
        "dragging": false,
        "id": "ConditionalRouter-RZ4gB",
        "measured": {
          "height": 464,
          "width": 320
        },
        "position": {
          "x": 629.8147473554036,
          "y": 794.122421261071
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ConditionalRouter-92JKj",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Routes input messages based on a conditional check.",
            "display_name": "Conditional Router",
            "documentation": "",
            "edited": true,
            "field_order": [
              "input_text",
              "operator",
              "match_text",
              "case_sensitive",
              "true_case_message",
              "false_case_message",
              "user_question",
              "max_iterations",
              "default_route"
            ],
            "frozen": false,
            "icon": "split",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Passthrough (if True)",
                "group_outputs": false,
                "hidden": null,
                "method": "text_response",
                "name": "passthrough_result",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "case_sensitive": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Case Sensitive",
                "dynamic": false,
                "info": "If true, the comparison will be case sensitive.",
                "list": false,
                "list_add_label": "Add More",
                "name": "case_sensitive",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import re\r\nfrom typing import Any, List\r\n\r\nfrom langflow.custom.custom_component.component import Component\r\nfrom langflow.io import (\r\n    BoolInput,\r\n    DropdownInput,\r\n    IntInput,\r\n    MessageInput,\r\n    MessageTextInput,\r\n    Output,\r\n)\r\nfrom langflow.schema.message import Message\r\n\r\n\r\nclass ConditionalRouterComponent(Component):\r\n    display_name = \"Conditional Router\"\r\n    description = \"Routes input messages based on a conditional check.\"\r\n    icon = \"split\"\r\n    name = \"ConditionalRouter\"\r\n\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.__iteration_updated = False\r\n\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"input_text\",\r\n            display_name=\"Text Input\",\r\n            info=\"The primary text input for the operation.\",\r\n            required=True,\r\n        ),\r\n        DropdownInput(\r\n            name=\"operator\",\r\n            display_name=\"Operator\",\r\n            options=[\r\n                \"equals\", \"not equals\", \"contains\", \"starts with\", \"ends with\", \"regex\",\r\n                \"less than\", \"less than or equal\", \"greater than\", \"greater than or equal\",\r\n            ],\r\n            info=\"The operator to apply for comparing the texts.\",\r\n            value=\"equals\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"match_text\",\r\n            display_name=\"Match Text\",\r\n            info=\"The text input to compare against.\",\r\n            required=True,\r\n        ),\r\n        BoolInput(\r\n            name=\"case_sensitive\",\r\n            display_name=\"Case Sensitive\",\r\n            info=\"If true, the comparison will be case sensitive.\",\r\n            value=True,\r\n            advanced=True,\r\n        ),\r\n        MessageInput(\r\n            name=\"true_case_message\",\r\n            display_name=\"True Case Message\",\r\n            info=\"The message to pass if the condition is True.\",\r\n            advanced=True,\r\n        ),\r\n        MessageInput(\r\n            name=\"false_case_message\",\r\n            display_name=\"False Case Message\",\r\n            info=\"The message to pass if the condition is False.\",\r\n            advanced=True,\r\n        ),\r\n        MessageInput(\r\n            name=\"user_question\",\r\n            display_name=\"Passthrough Message\",\r\n            info=\"The message to pass through if the condition is True.\",\r\n        ),\r\n        IntInput(\r\n            name=\"max_iterations\",\r\n            display_name=\"Max Iterations\",\r\n            info=\"The maximum number of iterations for the conditional router.\",\r\n            value=10,\r\n            advanced=True,\r\n        ),\r\n        DropdownInput(\r\n            name=\"default_route\",\r\n            display_name=\"Default Route\",\r\n            options=[\"true_result\", \"false_result\"],\r\n            info=\"The default route to take when max iterations are reached.\",\r\n            value=\"false_result\",\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Passthrough (if True)\", name=\"passthrough_result\", method=\"text_response\"),\r\n    ]\r\n\r\n    def _pre_run_setup(self):\r\n        self.__iteration_updated = False\r\n\r\n    def _to_str(self, v: Any) -> str:\r\n        if v is None:\r\n            return \"\"\r\n        if hasattr(v, \"content\"):\r\n            return str(v.content or \"\")\r\n        return str(v)\r\n\r\n    def evaluate_condition(self) -> bool:\r\n        input_s = self._to_str(self.input_text)\r\n        match_s = self._to_str(self.match_text)\r\n        operator = self.operator\r\n        case_sensitive = self.case_sensitive\r\n\r\n        if not case_sensitive and operator != \"regex\":\r\n            input_s = input_s.lower()\r\n            match_s = match_s.lower()\r\n\r\n        try:\r\n            if operator == \"equals\": return input_s == match_s\r\n            if operator == \"not equals\": return input_s != match_s\r\n            if operator == \"contains\": return match_s in input_s\r\n            if operator == \"starts with\": return input_s.startswith(match_s)\r\n            if operator == \"ends with\": return input_s.endswith(match_s)\r\n            if operator == \"regex\": return bool(re.search(match_s, input_s))\r\n            \r\n            if operator in [\"less than\", \"less than or equal\", \"greater than\", \"greater than or equal\"]:\r\n                input_num = float(input_s)\r\n                match_num = float(match_s)\r\n                if operator == \"less than\": return input_num < match_num\r\n                if operator == \"less than or equal\": return input_num <= match_num\r\n                if operator == \"greater than\": return input_num > match_num\r\n                if operator == \"greater than or equal\": return input_num >= match_num\r\n        except (ValueError, TypeError, re.error):\r\n            return False\r\n        return False\r\n\r\n    def iterate_and_stop_once(self, route_to_stop: str):\r\n        if not self.__iteration_updated:\r\n            self.update_ctx({f\"{self._id}_iteration\": self.ctx.get(f\"{self._id}_iteration\", 0) + 1})\r\n            self.__iteration_updated = True\r\n            if self.ctx.get(f\"{self._id}_iteration\", 0) >= self.max_iterations and route_to_stop == self.default_route:\r\n                route_to_stop = \"true_result\" if route_to_stop == \"false_result\" else \"false_result\"\r\n            self.stop(route_to_stop)\r\n\r\n    def true_response(self) -> Message:\r\n        if self.evaluate_condition():\r\n            self.status = self.true_case_message\r\n            self.iterate_and_stop_once(\"false_result\")\r\n            return self.true_case_message\r\n        self.iterate_and_stop_once(\"true_result\")\r\n        return Message(content=\"\")\r\n\r\n    def false_response(self) -> Message:\r\n        if not self.evaluate_condition():\r\n            self.status = self.false_case_message\r\n            self.iterate_and_stop_once(\"true_result\")\r\n            return self.false_case_message\r\n        self.iterate_and_stop_once(\"false_result\")\r\n        return Message(content=\"\")\r\n\r\n    def text_response(self) -> Message: # Ya no es Optional[Message]\r\n        if self.evaluate_condition():\r\n            self.status = \"Condition is True -> Passing message through.\"\r\n            return self.user_question\r\n    \r\n        # Si la condición es False, devuelve un mensaje VACÍO\r\n        self.status = \"Condition is False -> Passing an empty message.\"\r\n        return Message(content=\"\") # <-- Solución con texto vacío"
              },
              "default_route": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Default Route",
                "dynamic": false,
                "info": "The default route to take when max iterations are reached.",
                "name": "default_route",
                "options": [
                  "true_result",
                  "false_result"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "false_result"
              },
              "false_case_message": {
                "_input_type": "MessageInput",
                "advanced": true,
                "display_name": "False Case Message",
                "dynamic": false,
                "info": "The message to pass if the condition is False.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "false_case_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "input_text": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Text Input",
                "dynamic": false,
                "info": "The primary text input for the operation.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_text",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "match_text": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Match Text",
                "dynamic": false,
                "info": "The text input to compare against.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "match_text",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "GITHUB_INGESTED"
              },
              "max_iterations": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Iterations",
                "dynamic": false,
                "info": "The maximum number of iterations for the conditional router.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_iterations",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 10
              },
              "operator": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Operator",
                "dynamic": false,
                "info": "The operator to apply for comparing the texts.",
                "name": "operator",
                "options": [
                  "equals",
                  "not equals",
                  "contains",
                  "starts with",
                  "ends with",
                  "regex",
                  "less than",
                  "less than or equal",
                  "greater than",
                  "greater than or equal"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "equals"
              },
              "true_case_message": {
                "_input_type": "MessageInput",
                "advanced": true,
                "display_name": "True Case Message",
                "dynamic": false,
                "info": "The message to pass if the condition is True.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "true_case_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "user_question": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Passthrough Message",
                "dynamic": false,
                "info": "The message to pass through if the condition is True.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "user_question",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "true_result",
          "showNode": true,
          "type": "ConditionalRouter"
        },
        "dragging": false,
        "id": "ConditionalRouter-92JKj",
        "measured": {
          "height": 464,
          "width": 320
        },
        "position": {
          "x": 629.7829920399677,
          "y": 234.60675473719868
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ChatOutput-KRmEh",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "https://docs.langflow.org/components-io#chat-output",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.helpers.data import safe_convert\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.template.field.base import Output\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Output Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, _icon, display_name, source_id = self.get_properties_from_source_component()\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message):\n            message = self.input_value\n            # Update message properties\n            message.text = text\n        else:\n            message = Message(text=text)\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n\n        # Store message if needed\n        if self.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            clean_data: bool = getattr(self, \"clean_data\", False)\n            return \"\\n\".join([safe_convert(item, clean_data=clean_data) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return safe_convert(self.input_value)\n"
              },
              "data_template": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_value",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "external_options": {},
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "ChatOutput"
        },
        "dragging": false,
        "id": "ChatOutput-KRmEh",
        "measured": {
          "height": 164,
          "width": 320
        },
        "position": {
          "x": 3461.8472508538525,
          "y": 207.90430956010465
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "JigsawStackAISearch-OqFI6",
          "node": {
            "base_classes": [
              "Data",
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Effortlessly search the Web and get access to high-quality results powered with AI.",
            "display_name": "AI Web Search",
            "documentation": "https://jigsawstack.com/docs/api-reference/web/ai-search",
            "edited": true,
            "field_order": [
              "api_key",
              "query",
              "ai_overview",
              "safe_search",
              "spell_check",
              "number_of_results"
            ],
            "frozen": false,
            "icon": "JigsawStack",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "AI Search Results",
                "group_outputs": false,
                "hidden": null,
                "method": "search",
                "name": "search_results",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Content Text",
                "group_outputs": false,
                "hidden": null,
                "method": "get_content_text",
                "name": "content_text",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "ai_overview": {
                "_input_type": "BoolInput",
                "advanced": false,
                "display_name": "AI Overview",
                "dynamic": false,
                "info": "Include AI powered overview in the search results",
                "list": false,
                "list_add_label": "Add More",
                "name": "ai_overview",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "JigsawStack API Key",
                "dynamic": false,
                "info": "Your JigsawStack API key for authentication",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.custom.custom_component.component import Component\r\nfrom langflow.io import BoolInput, DropdownInput, IntInput, Output, QueryInput, SecretStrInput\r\nfrom langflow.schema.data import Data\r\nfrom langflow.schema.message import Message\r\n\r\n\r\nclass JigsawStackAIWebSearchComponent(Component):\r\n    display_name = \"AI Web Search\"\r\n    description = \"Effortlessly search the Web and get access to high-quality results powered with AI.\"\r\n    documentation = \"https://jigsawstack.com/docs/api-reference/web/ai-search\"\r\n    icon = \"JigsawStack\"\r\n    name = \"JigsawStackAISearch\"\r\n\r\n    inputs = [\r\n        SecretStrInput(\r\n            name=\"api_key\",\r\n            display_name=\"JigsawStack API Key\",\r\n            info=\"Your JigsawStack API key for authentication\",\r\n            required=True,\r\n        ),\r\n        QueryInput(\r\n            name=\"query\",\r\n            display_name=\"Query\",\r\n            info=\"The search value. The maximum query character length is 400\",\r\n            required=True,\r\n            tool_mode=True,\r\n        ),\r\n        BoolInput(\r\n            name=\"ai_overview\",\r\n            display_name=\"AI Overview\",\r\n            info=\"Include AI powered overview in the search results\",\r\n            required=False,\r\n            value=True,\r\n        ),\r\n        DropdownInput(\r\n            name=\"safe_search\",\r\n            display_name=\"Safe Search\",\r\n            info=\"Enable safe search to filter out adult content\",\r\n            required=False,\r\n            options=[\"moderate\", \"strict\", \"off\"],\r\n            value=\"off\",\r\n        ),\r\n        BoolInput(\r\n            name=\"spell_check\",\r\n            display_name=\"Spell Check\",\r\n            info=\"Spell check the search query\",\r\n            required=False,\r\n            value=True,\r\n        ),\r\n        IntInput(\r\n            name=\"number_of_results\",\r\n            display_name=\"Number of Results\",\r\n            info=\"Number of results to return.\",\r\n            required=True,\r\n            value=3,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"AI Search Results\", name=\"search_results\", method=\"search\"),\r\n        Output(display_name=\"Content Text\", name=\"content_text\", method=\"get_content_text\"),\r\n    ]\r\n\r\n    def _execute_search(self):\r\n        \"\"\"Método auxiliar para ejecutar la llamada a la API y manejar la importación.\"\"\"\r\n        try:\r\n            from jigsawstack import JigsawStack, JigsawStackError\r\n        except ImportError as e:\r\n            raise ImportError(\r\n                \"JigsawStack package not found. Install it with: pip install jigsawstack>=0.2.7\"\r\n            ) from e\r\n\r\n        client = JigsawStack(api_key=self.api_key)\r\n\r\n        # build request object\r\n        search_params = {\r\n            \"query\": self.query,\r\n            \"ai_overview\": self.ai_overview,\r\n            \"safe_search\": self.safe_search,\r\n            \"spell_check\": self.spell_check,\r\n        }\r\n        if self.number_of_results:\r\n            # La API de JigsawStack usa 'max_results'\r\n            search_params[\"max_results\"] = self.number_of_results\r\n\r\n        try:\r\n            response = client.web.search(search_params)\r\n            \r\n            if not response.get(\"success\", False):\r\n                raise ValueError(\"JigsawStack API returned unsuccessful response\")\r\n\r\n            return response, JigsawStackError\r\n            \r\n        except JigsawStackError as e:\r\n            # Re-lanzar o manejar el error en los métodos de salida\r\n            raise e\r\n        except Exception as e:\r\n            raise e\r\n\r\n    def search(self) -> Data:\r\n        \"\"\"Devuelve el objeto Data completo con los resultados de la búsqueda.\"\"\"\r\n        try:\r\n            response, JigsawStackError = self._execute_search()\r\n        except ImportError as e:\r\n            return Data(data={\"error\": str(e), \"success\": False})\r\n        except Exception as e:\r\n            self.status = f\"Error: {e!s}\"\r\n            return Data(data={\"error\": str(e), \"success\": False})\r\n\r\n        # Obtener los resultados y aplicar el CORTE EXPLÍCITO para asegurar la limitación.\r\n        results = response.get(\"results\", [])\r\n        # Aplica el slice para limitar la cantidad total de resultados (la corrección clave)\r\n        results = results[:self.number_of_results]\r\n\r\n        # Limitar links a máximo 3 por resultado\r\n        for r in results:\r\n            if \"links\" in r:\r\n                r[\"links\"] = r[\"links\"][:3]\r\n\r\n        result_data = {\r\n            \"query\": self.query,\r\n            \"ai_overview\": response.get(\"ai_overview\", \"\"),\r\n            \"spell_fixed\": response.get(\"spell_fixed\", False),\r\n            \"is_safe\": response.get(\"is_safe\", True),\r\n            \"results\": results, # Usamos la lista de resultados ya limitada\r\n            \"success\": True,\r\n        }\r\n\r\n        self.status = f\"Search complete for: {response.get('query', '')}\"\r\n        return Data(data=result_data)\r\n\r\n    def get_content_text(self) -> Message:\r\n        \"\"\"Devuelve solo el AI Overview como texto (formato Message).\"\"\"\r\n        try:\r\n            response, JigsawStackError = self._execute_search()\r\n        except ImportError:\r\n            return Message(text=\"Error: JigsawStack package not found.\")\r\n        except JigsawStackError as e:\r\n            return Message(text=f\"Error while using AI Search: {e!s}\")\r\n        except Exception as e:\r\n            return Message(text=f\"An unexpected error occurred: {e!s}\")\r\n\r\n        # La limitación de resultados se aplica en _execute_search si la API lo respeta\r\n        # y se vuelve a aplicar en 'search'. Para 'get_content_text' solo necesitamos\r\n        # el 'ai_overview'.\r\n\r\n        # Devolver solo el AI overview como texto\r\n        content = response.get(\"ai_overview\", \"\")\r\n        return Message(text=content)"
              },
              "number_of_results": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Number of Results",
                "dynamic": false,
                "info": "Number of results to return.",
                "list": false,
                "list_add_label": "Add More",
                "name": "number_of_results",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 3
              },
              "query": {
                "_input_type": "QueryInput",
                "advanced": false,
                "display_name": "Query",
                "dynamic": false,
                "info": "The search value. The maximum query character length is 400",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "query",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "query",
                "value": ""
              },
              "safe_search": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Safe Search",
                "dynamic": false,
                "info": "Enable safe search to filter out adult content",
                "name": "safe_search",
                "options": [
                  "moderate",
                  "strict",
                  "off"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "off"
              },
              "spell_check": {
                "_input_type": "BoolInput",
                "advanced": false,
                "display_name": "Spell Check",
                "dynamic": false,
                "info": "Spell check the search query",
                "list": false,
                "list_add_label": "Add More",
                "name": "spell_check",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "selected_output": "search_results",
          "showNode": true,
          "type": "JigsawStackAISearch"
        },
        "dragging": false,
        "id": "JigsawStackAISearch-OqFI6",
        "measured": {
          "height": 547,
          "width": 320
        },
        "position": {
          "x": 666.4679556354017,
          "y": -621.6184222133328
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "NvidiaRerankComponent-DqD1n",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Rerank documents using the NVIDIA API (Model: Llama-3.2 1B v2).",
            "display_name": "NVIDIA Rerank",
            "documentation": "",
            "edited": true,
            "field_order": [
              "search_query",
              "search_results",
              "top_n",
              "api_key",
              "base_url"
            ],
            "frozen": false,
            "icon": "NVIDIA",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Reranked Documents",
                "group_outputs": false,
                "hidden": null,
                "method": "compress_documents",
                "name": "reranked_documents",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "NVIDIA API Key",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "base_url": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Base URL",
                "dynamic": false,
                "info": "The base URL of the NVIDIA API.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "https://integrate.api.nvidia.com/v1"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.compressors.model import LCCompressorComponent\nfrom langflow.field_typing import BaseDocumentCompressor\nfrom langflow.inputs.inputs import SecretStrInput\nfrom langflow.io import StrInput\nfrom langflow.template.field.base import Output\n\n\nclass NvidiaRerankComponent(LCCompressorComponent):\n    display_name = \"NVIDIA Rerank\"\n    description = \"Rerank documents using the NVIDIA API (Model: Llama-3.2 1B v2).\"\n    icon = \"NVIDIA\"\n\n    # Se elimina el Dropdown para seleccionar el modelo\n    inputs = [\n        *LCCompressorComponent.inputs,\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"NVIDIA API Key\",\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            value=\"https://integrate.api.nvidia.com/v1\",\n            info=\"The base URL of the NVIDIA API.\",\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Reranked Documents\",\n            name=\"reranked_documents\",\n            # Se vuelve a usar el método original 'compress_documents'\n            method=\"compress_documents\",\n        ),\n    ]\n\n    def build_compressor(self) -> BaseDocumentCompressor:\n        try:\n            from langchain_nvidia_ai_endpoints import NVIDIARerank\n        except ImportError as e:\n            msg = \"Please install langchain-nvidia-ai-endpoints to use the NVIDIA model.\"\n            raise ImportError(msg) from e\n        \n        # El nombre del modelo se define directamente en el código\n        fixed_model_name = \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\n\n        return NVIDIARerank(\n            api_key=self.api_key, \n            model=fixed_model_name, \n            base_url=self.base_url, \n            top_n=self.top_n\n        )\n"
              },
              "search_query": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Search Query",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "search_query",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "search_results": {
                "_input_type": "DataInput",
                "advanced": false,
                "display_name": "Search Results",
                "dynamic": false,
                "info": "Search Results from a Vector Store.",
                "input_types": [
                  "Data"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "search_results",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "top_n": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Top N",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "top_n",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 5
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "NvidiaRerankComponent"
        },
        "dragging": false,
        "id": "NvidiaRerankComponent-DqD1n",
        "measured": {
          "height": 426,
          "width": 320
        },
        "position": {
          "x": 1732.0593389499484,
          "y": -69.99600817870997
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Chroma-n7Ava",
          "node": {
            "base_classes": [
              "Data",
              "DataFrame"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Chroma Vector Store with search and ingest capabilities",
            "display_name": "Chroma DB",
            "documentation": "",
            "edited": true,
            "field_order": [
              "collection_name",
              "persist_directory",
              "ingest_data",
              "search_query",
              "should_cache_vector_store",
              "embedding",
              "number_of_results",
              "search_type"
            ],
            "frozen": false,
            "icon": "Chroma",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Search Results",
                "group_outputs": false,
                "hidden": null,
                "method": "search_documents",
                "name": "search_results",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "DataFrame",
                "group_outputs": false,
                "hidden": null,
                "method": "as_dataframe",
                "name": "dataframe",
                "options": null,
                "required_inputs": null,
                "selected": "DataFrame",
                "tool_mode": true,
                "types": [
                  "DataFrame"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from copy import deepcopy\nfrom typing import TYPE_CHECKING, List, Dict, Any\nfrom pathlib import Path\n\nfrom chromadb.config import Settings\nfrom langchain_chroma import Chroma\nfrom typing_extensions import override\nimport pandas as pd\n\nfrom langchain_core.documents import Document\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.base.vectorstores.utils import chroma_collection_to_data\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, HandleInput, IntInput, StrInput\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.helpers.data import docs_to_data # <-- Importación añadida\n\nif TYPE_CHECKING:\n    from langflow.schema.dataframe import DataFrame\n\n\nclass ChromaVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"Chroma Vector Store with search and ingest capabilities.\"\"\"\n\n    display_name: str = \"Chroma DB\"\n    description: str = \"Chroma Vector Store with search and ingest capabilities\"\n    name = \"Chroma\"\n    icon = \"Chroma\"\n\n    # Los inputs no necesitan cambios\n    inputs = [\n        StrInput(name=\"collection_name\", display_name=\"Collection Name\", value=\"langflow\"),\n        StrInput(name=\"persist_directory\", display_name=\"Persist Directory\"),\n        *LCVectorStoreComponent.inputs,\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(name=\"number_of_results\", display_name=\"Number of Results\", info=\"Number of results to return.\", advanced=True, value=7),\n        DropdownInput(\n            name=\"search_type\",\n            display_name=\"Search Type\",\n            options=[\"Similarity\", \"MMR\"],\n            value=\"MMR\",\n            advanced=True,\n        ),\n        # ... (el resto de tus inputs)\n    ]\n\n    @override\n    @check_cached_vector_store\n    def build_vector_store(self) -> Chroma:\n        # La lógica de ingesta no cambia\n        persist_directory = self.resolve_path(self.persist_directory) if self.persist_directory else None\n        chroma = Chroma(\n            persist_directory=persist_directory,\n            embedding_function=self.embedding,\n            collection_name=self.collection_name,\n        )\n        self._add_documents_to_vector_store(chroma)\n        return chroma\n\n    def _sanitize_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:\n        # La lógica de sanitización no cambia\n        if not isinstance(metadata, dict):\n            return {}\n        return {key: value for key, value in metadata.items() if isinstance(value, (str, int, float, bool))}\n\n    def search_documents(self) -> List[Data]:\n        \"\"\"\n        Loads the Chroma DB index and searches for documents.\n        If the search query is empty, it does nothing.\n        \"\"\"\n        # 1. Cláusula de Guarda: si no hay pregunta, no se ejecuta.\n        if not self.search_query or not isinstance(self.search_query, str) or not self.search_query.strip():\n            self.status = \"No search query provided. Component will not run.\"\n            return []\n\n        # 2. Cargamos la base de datos desde el disco\n        persist_directory = self.resolve_path(self.persist_directory) if self.persist_directory else None\n        if not persist_directory or not Path(persist_directory).exists():\n             raise ValueError(f\"Persist directory '{persist_directory}' not found.\")\n\n        vector_store = Chroma(\n            persist_directory=persist_directory,\n            embedding_function=self.embedding,\n            collection_name=self.collection_name,\n        )\n        \n        # 3. Realizamos la búsqueda\n        if self.search_type.lower() == \"mmr\":\n            retriever = vector_store.as_retriever(\n                search_type=\"mmr\",\n                search_kwargs={\n                    \"k\": self.number_of_results,\n                    \"fetch_k\": max(self.number_of_results * 5, 20),\n                    \"lambda_mult\": 0.5,\n                },\n            )\n            docs = retriever.invoke(self.search_query)\n        else:\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n        \n        # 4. Convertimos los resultados al formato de Langflow\n        return docs_to_data(docs)"
              },
              "collection_name": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Collection Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "collection_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "github_collection"
              },
              "embedding": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Embedding",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Embeddings"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "embedding",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "ingest_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Ingest Data",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Data",
                  "DataFrame"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "ingest_data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "number_of_results": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Number of Results",
                "dynamic": false,
                "info": "Number of results to return.",
                "list": false,
                "list_add_label": "Add More",
                "name": "number_of_results",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 10
              },
              "persist_directory": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Persist Directory",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "persist_directory",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "./indices/github"
              },
              "search_query": {
                "_input_type": "QueryInput",
                "advanced": false,
                "display_name": "Search Query",
                "dynamic": false,
                "info": "Enter a query to run a similarity search.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "search_query",
                "placeholder": "Enter a query...",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "query",
                "value": ""
              },
              "search_type": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Search Type",
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "name": "search_type",
                "options": [
                  "Similarity",
                  "MMR"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Similarity"
              },
              "should_cache_vector_store": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Cache Vector Store",
                "dynamic": false,
                "info": "If True, the vector store will be cached for the current build of the component. This is useful for components that have multiple output methods and want to share the same vector store.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_cache_vector_store",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "selected_output": "search_results",
          "showNode": true,
          "type": "Chroma"
        },
        "dragging": false,
        "id": "Chroma-n7Ava",
        "measured": {
          "height": 470,
          "width": 320
        },
        "position": {
          "x": 1154.013056861263,
          "y": 258.824083689576
        },
        "selected": true,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Chroma-TRdau",
          "node": {
            "base_classes": [
              "Data",
              "DataFrame"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Chroma Vector Store with search and ingest capabilities",
            "display_name": "Chroma DB",
            "documentation": "",
            "edited": true,
            "field_order": [
              "collection_name",
              "persist_directory",
              "ingest_data",
              "search_query",
              "should_cache_vector_store",
              "embedding",
              "number_of_results",
              "search_type"
            ],
            "frozen": false,
            "icon": "Chroma",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Search Results",
                "group_outputs": false,
                "hidden": null,
                "method": "search_documents",
                "name": "search_results",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "DataFrame",
                "group_outputs": false,
                "hidden": null,
                "method": "as_dataframe",
                "name": "dataframe",
                "options": null,
                "required_inputs": null,
                "selected": "DataFrame",
                "tool_mode": true,
                "types": [
                  "DataFrame"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from copy import deepcopy\r\nfrom typing import TYPE_CHECKING, List, Dict, Any\r\nfrom pathlib import Path\r\n\r\nfrom chromadb.config import Settings\r\nfrom langchain_chroma import Chroma\r\nfrom typing_extensions import override\r\nimport pandas as pd\r\n\r\nfrom langchain_core.documents import Document\r\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\r\nfrom langflow.base.vectorstores.utils import chroma_collection_to_data\r\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, HandleInput, IntInput, StrInput\r\nfrom langflow.schema.data import Data\r\nfrom langflow.schema.dataframe import DataFrame\r\nfrom langflow.helpers.data import docs_to_data # <-- Importación añadida\r\n\r\nif TYPE_CHECKING:\r\n    from langflow.schema.dataframe import DataFrame\r\n\r\n\r\nclass ChromaVectorStoreComponent(LCVectorStoreComponent):\r\n    \"\"\"Chroma Vector Store with search and ingest capabilities.\"\"\"\r\n\r\n    display_name: str = \"Chroma DB\"\r\n    description: str = \"Chroma Vector Store with search and ingest capabilities\"\r\n    name = \"Chroma\"\r\n    icon = \"Chroma\"\r\n\r\n    inputs = [\r\n        StrInput(name=\"collection_name\", display_name=\"Collection Name\", value=\"langflow\"),\r\n        StrInput(name=\"persist_directory\", display_name=\"Persist Directory\"),\r\n        *LCVectorStoreComponent.inputs,\r\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\r\n        IntInput(name=\"number_of_results\", display_name=\"Number of Results\", info=\"Number of results to return.\", advanced=True, value=7),\r\n        DropdownInput(\r\n            name=\"search_type\",\r\n            display_name=\"Search Type\",\r\n            options=[\"Similarity\", \"MMR\"],\r\n            value=\"MMR\",\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    @override\r\n    @check_cached_vector_store\r\n    def build_vector_store(self) -> Chroma:\r\n        # La lógica de ingesta no cambia\r\n        persist_directory = self.resolve_path(self.persist_directory) if self.persist_directory else None\r\n        chroma = Chroma(\r\n            persist_directory=persist_directory,\r\n            embedding_function=self.embedding,\r\n            collection_name=self.collection_name,\r\n        )\r\n        self._add_documents_to_vector_store(chroma)\r\n        return chroma\r\n\r\n    def _sanitize_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:\r\n        # La lógica de sanitización no cambia\r\n        if not isinstance(metadata, dict):\r\n            return {}\r\n        return {key: value for key, value in metadata.items() if isinstance(value, (str, int, float, bool))}\r\n\r\n    def search_documents(self) -> List[Data]:\r\n        \"\"\"\r\n        Loads the Chroma DB index and searches for documents.\r\n        If the search query is empty, it does nothing.\r\n        \"\"\"\r\n        # 1. Cláusula de Guarda: si no hay pregunta, no se ejecuta.\r\n        if not self.search_query or not isinstance(self.search_query, str) or not self.search_query.strip():\r\n            self.status = \"No search query provided. Component will not run.\"\r\n            return []\r\n\r\n        # 2. Cargamos la base de datos desde el disco\r\n        persist_directory = self.resolve_path(self.persist_directory) if self.persist_directory else None\r\n        if not persist_directory or not Path(persist_directory).exists():\r\n             raise ValueError(f\"Persist directory '{persist_directory}' not found.\")\r\n\r\n        vector_store = Chroma(\r\n            persist_directory=persist_directory,\r\n            embedding_function=self.embedding,\r\n            collection_name=self.collection_name,\r\n        )\r\n        \r\n         # 3. Realizamos la búsqueda\r\n        if self.search_type.lower() == \"mmr\":\r\n            retriever = vector_store.as_retriever(\r\n                search_type=\"mmr\",\r\n                search_kwargs={\r\n                    \"k\": self.number_of_results,\r\n                    \"fetch_k\": max(self.number_of_results * 5, 20),\r\n                    \"lambda_mult\": 0.5,\r\n                },\r\n            )\r\n            docs = retriever.invoke(self.search_query)\r\n        else:\r\n            docs = vector_store.similarity_search(\r\n                query=self.search_query,\r\n                k=self.number_of_results,\r\n            )\r\n\r\n        \r\n        # 4. Convertimos los resultados al formato de Langflow\r\n        return docs_to_data(docs)"
              },
              "collection_name": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Collection Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "collection_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "pdf_collection"
              },
              "embedding": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Embedding",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Embeddings"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "embedding",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "ingest_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Ingest Data",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Data",
                  "DataFrame"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "ingest_data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "number_of_results": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Number of Results",
                "dynamic": false,
                "info": "Number of results to return.",
                "list": false,
                "list_add_label": "Add More",
                "name": "number_of_results",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 10
              },
              "persist_directory": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Persist Directory",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "persist_directory",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "./indices/pdf"
              },
              "search_query": {
                "_input_type": "QueryInput",
                "advanced": false,
                "display_name": "Search Query",
                "dynamic": false,
                "info": "Enter a query to run a similarity search.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "search_query",
                "placeholder": "Enter a query...",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "query",
                "value": ""
              },
              "search_type": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Search Type",
                "dynamic": false,
                "info": "",
                "name": "search_type",
                "options": [
                  "Similarity",
                  "MMR"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "MMR"
              },
              "should_cache_vector_store": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Cache Vector Store",
                "dynamic": false,
                "info": "If True, the vector store will be cached for the current build of the component. This is useful for components that have multiple output methods and want to share the same vector store.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_cache_vector_store",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "selected_output": "search_results",
          "showNode": true,
          "type": "Chroma"
        },
        "dragging": false,
        "id": "Chroma-TRdau",
        "measured": {
          "height": 470,
          "width": 320
        },
        "position": {
          "x": 1164.8946575143912,
          "y": 827.212221957757
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "NVIDIAEmbeddingsComponent-XGuHe",
          "node": {
            "base_classes": [
              "Embeddings"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generate embeddings using NVIDIA models.",
            "display_name": "NVIDIA Embeddings",
            "documentation": "",
            "edited": true,
            "field_order": [
              "base_url",
              "nvidia_api_key",
              "temperature"
            ],
            "frozen": false,
            "icon": "NVIDIA",
            "last_updated": "2025-10-27T02:11:40.287Z",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Embedding Model",
                "group_outputs": false,
                "hidden": null,
                "method": "build_embeddings",
                "name": "embeddings",
                "options": null,
                "required_inputs": null,
                "selected": "Embeddings",
                "tool_mode": true,
                "types": [
                  "Embeddings"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "base_url": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "NVIDIA Base URL",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "placeholder": "",
                "refresh_button": true,
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "https://integrate.api.nvidia.com/v1"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\n\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.inputs.inputs import DropdownInput, SecretStrInput\nfrom langflow.io import FloatInput, MessageTextInput\nfrom langflow.schema.dotdict import dotdict\n\n\nclass NVIDIAEmbeddingsComponent(LCEmbeddingsModel):\n    display_name: str = \"NVIDIA Embeddings\"\n    description: str = \"Generate embeddings using NVIDIA models.\"\n    icon = \"NVIDIA\"\n    model_name: str = \"nvidia/nv-embed-v1\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"NVIDIA Base URL\",\n            refresh_button=True,\n            value=\"https://integrate.api.nvidia.com/v1\",\n            required=True,\n        ),\n        SecretStrInput(\n            name=\"nvidia_api_key\",\n            display_name=\"NVIDIA API Key\",\n            info=\"The NVIDIA API Key.\",\n            advanced=False,\n            value=\"NVIDIA_API_KEY\",\n            required=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name == \"base_url\" and field_value:\n            try:\n                build_model = self.build_embeddings()\n                ids = [model.id for model in build_model.available_models]\n                build_config[\"model\"][\"options\"] = ids\n                build_config[\"model\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n        except ImportError as e:\n            msg = \"Please install langchain-nvidia-ai-endpoints to use the Nvidia model.\"\n            raise ImportError(msg) from e\n        try:\n            output = NVIDIAEmbeddings(\n                model=self.model_name,\n                base_url=self.base_url,\n                temperature=self.temperature,\n                nvidia_api_key=self.nvidia_api_key,\n            )\n        except Exception as e:\n            msg = f\"Could not connect to NVIDIA API. Error: {e}\"\n            raise ValueError(msg) from e\n        return output\n"
              },
              "nvidia_api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "NVIDIA API Key",
                "dynamic": false,
                "info": "The NVIDIA API Key.",
                "input_types": [],
                "load_from_db": false,
                "name": "nvidia_api_key",
                "password": true,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Model Temperature",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "temperature",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.1
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "NVIDIAEmbeddingsComponent"
        },
        "dragging": false,
        "id": "NVIDIAEmbeddingsComponent-XGuHe",
        "measured": {
          "height": 284,
          "width": 320
        },
        "position": {
          "x": 640.566876273496,
          "y": 1331.2247485187286
        },
        "selected": false,
        "type": "genericNode"
      }
    ],
    "viewport": {
      "x": -142.72198415036667,
      "y": 2.8871055008883673,
      "zoom": 0.45365662738983625
    }
  },
  "description": "Load your data for chat context with Retrieval Augmented Generation.",
  "endpoint_name": "retriever_flow",
  "id": "33e2a7a7-9930-4aec-b60d-ee63c0c8aee5",
  "is_component": false,
  "last_tested_version": "1.5.1",
  "name": "Retriever Flow",
  "tags": [
    "openai",
    "astradb",
    "rag",
    "q-a"
  ]
}