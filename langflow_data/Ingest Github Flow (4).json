{
  "data": {
    "edges": [
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LocalHuggingFaceEmbeddings",
            "id": "LocalHuggingFaceEmbeddings-1ppaO",
            "name": "embeddings",
            "output_types": [
              "Embeddings"
            ]
          },
          "targetHandle": {
            "fieldName": "embedding",
            "id": "FAISS-UuM7K",
            "inputTypes": [
              "Embeddings"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__LocalHuggingFaceEmbeddings-1ppaO{œdataTypeœ:œLocalHuggingFaceEmbeddingsœ,œidœ:œLocalHuggingFaceEmbeddings-1ppaOœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-FAISS-UuM7K{œfieldNameœ:œembeddingœ,œidœ:œFAISS-UuM7Kœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "LocalHuggingFaceEmbeddings-1ppaO",
        "sourceHandle": "{œdataTypeœ:œLocalHuggingFaceEmbeddingsœ,œidœ:œLocalHuggingFaceEmbeddings-1ppaOœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}",
        "target": "FAISS-UuM7K",
        "targetHandle": "{œfieldNameœ:œembeddingœ,œidœ:œFAISS-UuM7Kœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "SplitText",
            "id": "SplitText-TzwXF",
            "name": "dataframe",
            "output_types": [
              "DataFrame"
            ]
          },
          "targetHandle": {
            "fieldName": "ingest_data",
            "id": "FAISS-UuM7K",
            "inputTypes": [
              "Data",
              "DataFrame"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__SplitText-TzwXF{œdataTypeœ:œSplitTextœ,œidœ:œSplitText-TzwXFœ,œnameœ:œdataframeœ,œoutput_typesœ:[œDataFrameœ]}-FAISS-UuM7K{œfieldNameœ:œingest_dataœ,œidœ:œFAISS-UuM7Kœ,œinputTypesœ:[œDataœ,œDataFrameœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "SplitText-TzwXF",
        "sourceHandle": "{œdataTypeœ:œSplitTextœ,œidœ:œSplitText-TzwXFœ,œnameœ:œdataframeœ,œoutput_typesœ:[œDataFrameœ]}",
        "target": "FAISS-UuM7K",
        "targetHandle": "{œfieldNameœ:œingest_dataœ,œidœ:œFAISS-UuM7Kœ,œinputTypesœ:[œDataœ,œDataFrameœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "GitExtractorComponent",
            "id": "GitExtractorComponent-PmJhm",
            "name": "files_content",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "input_documents",
            "id": "MetadataTagger-XqRJY",
            "inputTypes": [
              "Data",
              "Message"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__GitExtractorComponent-PmJhm{œdataTypeœ:œGitExtractorComponentœ,œidœ:œGitExtractorComponent-PmJhmœ,œnameœ:œfiles_contentœ,œoutput_typesœ:[œDataœ]}-MetadataTagger-XqRJY{œfieldNameœ:œinput_documentsœ,œidœ:œMetadataTagger-XqRJYœ,œinputTypesœ:[œDataœ,œMessageœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "GitExtractorComponent-PmJhm",
        "sourceHandle": "{œdataTypeœ:œGitExtractorComponentœ,œidœ:œGitExtractorComponent-PmJhmœ,œnameœ:œfiles_contentœ,œoutput_typesœ:[œDataœ]}",
        "target": "MetadataTagger-XqRJY",
        "targetHandle": "{œfieldNameœ:œinput_documentsœ,œidœ:œMetadataTagger-XqRJYœ,œinputTypesœ:[œDataœ,œMessageœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "MetadataTagger",
            "id": "MetadataTagger-XqRJY",
            "name": "tagged_documents",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "input_documents",
            "id": "CustomComponent-F1TVs",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__MetadataTagger-XqRJY{œdataTypeœ:œMetadataTaggerœ,œidœ:œMetadataTagger-XqRJYœ,œnameœ:œtagged_documentsœ,œoutput_typesœ:[œDataœ]}-CustomComponent-F1TVs{œfieldNameœ:œinput_documentsœ,œidœ:œCustomComponent-F1TVsœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "MetadataTagger-XqRJY",
        "sourceHandle": "{œdataTypeœ:œMetadataTaggerœ,œidœ:œMetadataTagger-XqRJYœ,œnameœ:œtagged_documentsœ,œoutput_typesœ:[œDataœ]}",
        "target": "CustomComponent-F1TVs",
        "targetHandle": "{œfieldNameœ:œinput_documentsœ,œidœ:œCustomComponent-F1TVsœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "DocumentNormalizer",
            "id": "CustomComponent-F1TVs",
            "name": "normalized_documents",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "data_inputs",
            "id": "SplitText-TzwXF",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__CustomComponent-F1TVs{œdataTypeœ:œDocumentNormalizerœ,œidœ:œCustomComponent-F1TVsœ,œnameœ:œnormalized_documentsœ,œoutput_typesœ:[œDataœ]}-SplitText-TzwXF{œfieldNameœ:œdata_inputsœ,œidœ:œSplitText-TzwXFœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "CustomComponent-F1TVs",
        "sourceHandle": "{œdataTypeœ:œDocumentNormalizerœ,œidœ:œCustomComponent-F1TVsœ,œnameœ:œnormalized_documentsœ,œoutput_typesœ:[œDataœ]}",
        "target": "SplitText-TzwXF",
        "targetHandle": "{œfieldNameœ:œdata_inputsœ,œidœ:œSplitText-TzwXFœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}"
      }
    ],
    "nodes": [
      {
        "data": {
          "id": "LocalHuggingFaceEmbeddings-1ppaO",
          "node": {
            "base_classes": [
              "Embeddings"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Usa modelos HuggingFace locales para generar embeddings.",
            "display_name": "Local HuggingFace Embeddings",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "edited": true,
            "field_order": [
              "model_name"
            ],
            "frozen": false,
            "icon": "brain-circuit",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Embeddings",
                "group_outputs": false,
                "hidden": null,
                "method": "build_output",
                "name": "embeddings",
                "options": null,
                "required_inputs": null,
                "selected": "Embeddings",
                "tool_mode": true,
                "types": [
                  "Embeddings"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.custom.custom_component.component import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.data import Data\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.schema.embeddings import Embeddings\n\n\nclass CustomComponent(Component):\n    display_name = \"Local HuggingFace Embeddings\"\n    description = \"Usa modelos HuggingFace locales para generar embeddings.\"\n    documentation: str = \"https://docs.langflow.org/components-custom-components\"\n    icon = \"brain-circuit\"\n    name = \"LocalHuggingFaceEmbeddings\"\n\n    # -------- Inputs --------\n    inputs = [\n        MessageTextInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            info=\"Nombre del modelo HuggingFace a usar\",\n            value=\"BAAI/bge-small-en-v1.5\",  # Valor por defecto\n            tool_mode=True,\n        ),\n    ]\n\n    # -------- Outputs --------\n    outputs = [\n        Output(\n            display_name=\"Embeddings\",\n            name=\"embeddings\",\n            method=\"build_output\",\n        ),\n    ]\n\n    # -------- Lógica principal --------\n    def build_output(self) -> Embeddings:\n        model_name = self.model_name\n        embeddings = HuggingFaceEmbeddings(\n            model_name=model_name,\n            encode_kwargs={\"normalize_embeddings\": True},\n        )\n        self.status = f\"Modelo cargado: {model_name}\"\n        return embeddings\n"
              },
              "model_name": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Model Name",
                "dynamic": false,
                "info": "Nombre del modelo HuggingFace a usar",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "model_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "BAAI/bge-small-en-v1.5"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "LocalHuggingFaceEmbeddings"
        },
        "dragging": false,
        "id": "LocalHuggingFaceEmbeddings-1ppaO",
        "measured": {
          "height": 219,
          "width": 320
        },
        "position": {
          "x": 773.7279474376274,
          "y": 426.91100803121384
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "FAISS-UuM7K",
          "node": {
            "base_classes": [
              "Data",
              "DataFrame"
            ],
            "beta": false,
            "category": "vectorstores",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "FAISS Vector Store with search capabilities",
            "display_name": "FAISS",
            "documentation": "",
            "edited": false,
            "field_order": [
              "index_name",
              "persist_directory",
              "ingest_data",
              "search_query",
              "should_cache_vector_store",
              "allow_dangerous_deserialization",
              "embedding",
              "number_of_results"
            ],
            "frozen": false,
            "icon": "FAISS",
            "key": "FAISS",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Search Results",
                "group_outputs": false,
                "method": "search_documents",
                "name": "search_results",
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "DataFrame",
                "group_outputs": false,
                "method": "as_dataframe",
                "name": "dataframe",
                "tool_mode": true,
                "types": [
                  "DataFrame"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 0.0000597035286583837,
            "template": {
              "_type": "Component",
              "allow_dangerous_deserialization": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Allow Dangerous Deserialization",
                "dynamic": false,
                "info": "Set to True to allow loading pickle files from untrusted sources. Only enable this if you trust the source of the data.",
                "list": false,
                "list_add_label": "Add More",
                "name": "allow_dangerous_deserialization",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from pathlib import Path\n\nfrom langchain_community.vectorstores import FAISS\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import BoolInput, HandleInput, IntInput, StrInput\nfrom langflow.schema.data import Data\n\n\nclass FaissVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"FAISS Vector Store with search capabilities.\"\"\"\n\n    display_name: str = \"FAISS\"\n    description: str = \"FAISS Vector Store with search capabilities\"\n    name = \"FAISS\"\n    icon = \"FAISS\"\n\n    inputs = [\n        StrInput(\n            name=\"index_name\",\n            display_name=\"Index Name\",\n            value=\"langflow_index\",\n        ),\n        StrInput(\n            name=\"persist_directory\",\n            display_name=\"Persist Directory\",\n            info=\"Path to save the FAISS index. It will be relative to where Langflow is running.\",\n        ),\n        *LCVectorStoreComponent.inputs,\n        BoolInput(\n            name=\"allow_dangerous_deserialization\",\n            display_name=\"Allow Dangerous Deserialization\",\n            info=\"Set to True to allow loading pickle files from untrusted sources. \"\n            \"Only enable this if you trust the source of the data.\",\n            advanced=True,\n            value=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            advanced=True,\n            value=4,\n        ),\n    ]\n\n    @staticmethod\n    def resolve_path(path: str) -> str:\n        \"\"\"Resolve the path relative to the Langflow root.\n\n        Args:\n            path: The path to resolve\n        Returns:\n            str: The resolved path as a string\n        \"\"\"\n        return str(Path(path).resolve())\n\n    def get_persist_directory(self) -> Path:\n        \"\"\"Returns the resolved persist directory path or the current directory if not set.\"\"\"\n        if self.persist_directory:\n            return Path(self.resolve_path(self.persist_directory))\n        return Path()\n\n    @check_cached_vector_store\n    def build_vector_store(self) -> FAISS:\n        \"\"\"Builds the FAISS object.\"\"\"\n        path = self.get_persist_directory()\n        path.mkdir(parents=True, exist_ok=True)\n\n        # Convert DataFrame to Data if needed using parent's method\n        self.ingest_data = self._prepare_ingest_data()\n\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n\n        faiss = FAISS.from_documents(documents=documents, embedding=self.embedding)\n        faiss.save_local(str(path), self.index_name)\n        return faiss\n\n    def search_documents(self) -> list[Data]:\n        \"\"\"Search for documents in the FAISS vector store.\"\"\"\n        path = self.get_persist_directory()\n        index_path = path / f\"{self.index_name}.faiss\"\n\n        if not index_path.exists():\n            vector_store = self.build_vector_store()\n        else:\n            vector_store = FAISS.load_local(\n                folder_path=str(path),\n                embeddings=self.embedding,\n                index_name=self.index_name,\n                allow_dangerous_deserialization=self.allow_dangerous_deserialization,\n            )\n\n        if not vector_store:\n            msg = \"Failed to load the FAISS index.\"\n            raise ValueError(msg)\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n            return docs_to_data(docs)\n        return []\n"
              },
              "embedding": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Embedding",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Embeddings"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "embedding",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "index_name": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Index Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "index_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "index_github"
              },
              "ingest_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Ingest Data",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Data",
                  "DataFrame"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "ingest_data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "number_of_results": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Number of Results",
                "dynamic": false,
                "info": "Number of results to return.",
                "list": false,
                "list_add_label": "Add More",
                "name": "number_of_results",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 4
              },
              "persist_directory": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Persist Directory",
                "dynamic": false,
                "info": "Path to save the FAISS index. It will be relative to where Langflow is running.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "persist_directory",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "./indices/github"
              },
              "search_query": {
                "_input_type": "QueryInput",
                "advanced": false,
                "display_name": "Search Query",
                "dynamic": false,
                "info": "Enter a query to run a similarity search.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "search_query",
                "placeholder": "Enter a query...",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "query",
                "value": ""
              },
              "should_cache_vector_store": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Cache Vector Store",
                "dynamic": false,
                "info": "If True, the vector store will be cached for the current build of the component. This is useful for components that have multiple output methods and want to share the same vector store.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_cache_vector_store",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "selected_output": "search_results",
          "showNode": true,
          "type": "FAISS"
        },
        "dragging": false,
        "id": "FAISS-UuM7K",
        "measured": {
          "height": 455,
          "width": 320
        },
        "position": {
          "x": 1138.989936989,
          "y": 150.48739987633726
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "SplitText-TzwXF",
          "node": {
            "base_classes": [
              "DataFrame"
            ],
            "beta": false,
            "category": "processing",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Split text into chunks based on specified criteria.",
            "display_name": "Split Text",
            "documentation": "https://docs.langflow.org/components-processing#split-text",
            "edited": false,
            "field_order": [
              "data_inputs",
              "chunk_overlap",
              "chunk_size",
              "separator",
              "text_key",
              "keep_separator"
            ],
            "frozen": false,
            "icon": "scissors-line-dashed",
            "key": "SplitText",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Chunks",
                "group_outputs": false,
                "method": "split_text",
                "name": "dataframe",
                "selected": "DataFrame",
                "tool_mode": true,
                "types": [
                  "DataFrame"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 0.0006561452663029057,
            "template": {
              "_type": "Component",
              "chunk_overlap": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Chunk Overlap",
                "dynamic": false,
                "info": "Number of characters to overlap between chunks.",
                "list": false,
                "list_add_label": "Add More",
                "name": "chunk_overlap",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 200
              },
              "chunk_size": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Chunk Size",
                "dynamic": false,
                "info": "The maximum length of each chunk. Text is first split by separator, then chunks are merged up to this size. Individual splits larger than this won't be further divided.",
                "list": false,
                "list_add_label": "Add More",
                "name": "chunk_size",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1000
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langchain_text_splitters import CharacterTextSplitter\n\nfrom langflow.custom.custom_component.component import Component\nfrom langflow.io import DropdownInput, HandleInput, IntInput, MessageTextInput, Output\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.utils.util import unescape_string\n\n\nclass SplitTextComponent(Component):\n    display_name: str = \"Split Text\"\n    description: str = \"Split text into chunks based on specified criteria.\"\n    documentation: str = \"https://docs.langflow.org/components-processing#split-text\"\n    icon = \"scissors-line-dashed\"\n    name = \"SplitText\"\n\n    inputs = [\n        HandleInput(\n            name=\"data_inputs\",\n            display_name=\"Input\",\n            info=\"The data with texts to split in chunks.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"Number of characters to overlap between chunks.\",\n            value=200,\n        ),\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=(\n                \"The maximum length of each chunk. Text is first split by separator, \"\n                \"then chunks are merged up to this size. \"\n                \"Individual splits larger than this won't be further divided.\"\n            ),\n            value=1000,\n        ),\n        MessageTextInput(\n            name=\"separator\",\n            display_name=\"Separator\",\n            info=(\n                \"The character to split on. Use \\\\n for newline. \"\n                \"Examples: \\\\n\\\\n for paragraphs, \\\\n for lines, . for sentences\"\n            ),\n            value=\"\\n\",\n        ),\n        MessageTextInput(\n            name=\"text_key\",\n            display_name=\"Text Key\",\n            info=\"The key to use for the text column.\",\n            value=\"text\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"keep_separator\",\n            display_name=\"Keep Separator\",\n            info=\"Whether to keep the separator in the output chunks and where to place it.\",\n            options=[\"False\", \"True\", \"Start\", \"End\"],\n            value=\"False\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Chunks\", name=\"dataframe\", method=\"split_text\"),\n    ]\n\n    def _docs_to_data(self, docs) -> list[Data]:\n        return [Data(text=doc.page_content, data=doc.metadata) for doc in docs]\n\n    def _fix_separator(self, separator: str) -> str:\n        \"\"\"Fix common separator issues and convert to proper format.\"\"\"\n        if separator == \"/n\":\n            return \"\\n\"\n        if separator == \"/t\":\n            return \"\\t\"\n        return separator\n\n    def split_text_base(self):\n        separator = self._fix_separator(self.separator)\n        separator = unescape_string(separator)\n\n        if isinstance(self.data_inputs, DataFrame):\n            if not len(self.data_inputs):\n                msg = \"DataFrame is empty\"\n                raise TypeError(msg)\n\n            self.data_inputs.text_key = self.text_key\n            try:\n                documents = self.data_inputs.to_lc_documents()\n            except Exception as e:\n                msg = f\"Error converting DataFrame to documents: {e}\"\n                raise TypeError(msg) from e\n        elif isinstance(self.data_inputs, Message):\n            self.data_inputs = [self.data_inputs.to_data()]\n            return self.split_text_base()\n        else:\n            if not self.data_inputs:\n                msg = \"No data inputs provided\"\n                raise TypeError(msg)\n\n            documents = []\n            if isinstance(self.data_inputs, Data):\n                self.data_inputs.text_key = self.text_key\n                documents = [self.data_inputs.to_lc_document()]\n            else:\n                try:\n                    documents = [input_.to_lc_document() for input_ in self.data_inputs if isinstance(input_, Data)]\n                    if not documents:\n                        msg = f\"No valid Data inputs found in {type(self.data_inputs)}\"\n                        raise TypeError(msg)\n                except AttributeError as e:\n                    msg = f\"Invalid input type in collection: {e}\"\n                    raise TypeError(msg) from e\n        try:\n            # Convert string 'False'/'True' to boolean\n            keep_sep = self.keep_separator\n            if isinstance(keep_sep, str):\n                if keep_sep.lower() == \"false\":\n                    keep_sep = False\n                elif keep_sep.lower() == \"true\":\n                    keep_sep = True\n                # 'start' and 'end' are kept as strings\n\n            splitter = CharacterTextSplitter(\n                chunk_overlap=self.chunk_overlap,\n                chunk_size=self.chunk_size,\n                separator=separator,\n                keep_separator=keep_sep,\n            )\n            return splitter.split_documents(documents)\n        except Exception as e:\n            msg = f\"Error splitting text: {e}\"\n            raise TypeError(msg) from e\n\n    def split_text(self) -> DataFrame:\n        return DataFrame(self._docs_to_data(self.split_text_base()))\n"
              },
              "data_inputs": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "The data with texts to split in chunks.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "data_inputs",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "keep_separator": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Keep Separator",
                "dynamic": false,
                "external_options": {},
                "info": "Whether to keep the separator in the output chunks and where to place it.",
                "name": "keep_separator",
                "options": [
                  "False",
                  "True",
                  "Start",
                  "End"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "False"
              },
              "separator": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Separator",
                "dynamic": false,
                "info": "The character to split on. Use \\n for newline. Examples: \\n\\n for paragraphs, \\n for lines, . for sentences",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "separator",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "\n"
              },
              "text_key": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Text Key",
                "dynamic": false,
                "info": "The key to use for the text column.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "text_key",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "content"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "SplitText"
        },
        "dragging": false,
        "id": "SplitText-TzwXF",
        "measured": {
          "height": 411,
          "width": 320
        },
        "position": {
          "x": 781.0842582377572,
          "y": -73.46391286815134
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "GitExtractorComponent-PmJhm",
          "node": {
            "base_classes": [
              "Data",
              "Message"
            ],
            "beta": false,
            "category": "git",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Analyzes a Git repository and returns file contents and complete repository information",
            "display_name": "GitExtractor",
            "documentation": "",
            "edited": false,
            "field_order": [
              "repository_url"
            ],
            "frozen": false,
            "icon": "GitLoader",
            "key": "GitExtractorComponent",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Text-Based File Contents",
                "group_outputs": false,
                "method": "get_text_based_file_contents",
                "name": "text_based_file_contents",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Directory Structure",
                "group_outputs": false,
                "method": "get_directory_structure",
                "name": "directory_structure",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Repository Info",
                "group_outputs": false,
                "method": "get_repository_info",
                "name": "repository_info",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Statistics",
                "group_outputs": false,
                "method": "get_statistics",
                "name": "statistics",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Files Content",
                "group_outputs": false,
                "method": "get_files_content",
                "name": "files_content",
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 1.1732828199964098e-19,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import os\nimport shutil\nimport tempfile\nfrom contextlib import asynccontextmanager\nfrom pathlib import Path\n\nimport aiofiles\nimport git\n\nfrom langflow.custom.custom_component.component import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.data import Data\nfrom langflow.schema.message import Message\n\n\nclass GitExtractorComponent(Component):\n    display_name = \"GitExtractor\"\n    description = \"Analyzes a Git repository and returns file contents and complete repository information\"\n    icon = \"GitLoader\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"repository_url\",\n            display_name=\"Repository URL\",\n            info=\"URL of the Git repository (e.g., https://github.com/username/repo)\",\n            value=\"\",\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Text-Based File Contents\",\n            name=\"text_based_file_contents\",\n            method=\"get_text_based_file_contents\",\n        ),\n        Output(display_name=\"Directory Structure\", name=\"directory_structure\", method=\"get_directory_structure\"),\n        Output(display_name=\"Repository Info\", name=\"repository_info\", method=\"get_repository_info\"),\n        Output(display_name=\"Statistics\", name=\"statistics\", method=\"get_statistics\"),\n        Output(display_name=\"Files Content\", name=\"files_content\", method=\"get_files_content\"),\n    ]\n\n    @asynccontextmanager\n    async def temp_git_repo(self):\n        \"\"\"Async context manager for temporary git repository cloning.\"\"\"\n        temp_dir = tempfile.mkdtemp()\n        try:\n            # Clone is still sync but wrapped in try/finally\n            git.Repo.clone_from(self.repository_url, temp_dir)\n            yield temp_dir\n        finally:\n            shutil.rmtree(temp_dir, ignore_errors=True)\n\n    async def get_repository_info(self) -> list[Data]:\n        try:\n            async with self.temp_git_repo() as temp_dir:\n                repo = git.Repo(temp_dir)\n                repo_info = {\n                    \"name\": self.repository_url.split(\"/\")[-1],\n                    \"url\": self.repository_url,\n                    \"default_branch\": repo.active_branch.name,\n                    \"remote_urls\": [remote.url for remote in repo.remotes],\n                    \"last_commit\": {\n                        \"hash\": repo.head.commit.hexsha,\n                        \"author\": str(repo.head.commit.author),\n                        \"message\": repo.head.commit.message.strip(),\n                        \"date\": str(repo.head.commit.committed_datetime),\n                    },\n                    \"branches\": [str(branch) for branch in repo.branches],\n                }\n                result = [Data(data=repo_info)]\n                self.status = result\n                return result\n        except git.GitError as e:\n            error_result = [Data(data={\"error\": f\"Error getting repository info: {e!s}\"})]\n            self.status = error_result\n            return error_result\n\n    async def get_statistics(self) -> list[Data]:\n        try:\n            async with self.temp_git_repo() as temp_dir:\n                total_files = 0\n                total_size = 0\n                total_lines = 0\n                binary_files = 0\n                directories = 0\n\n                for root, dirs, files in os.walk(temp_dir):\n                    total_files += len(files)\n                    directories += len(dirs)\n                    for file in files:\n                        file_path = Path(root) / file\n                        total_size += file_path.stat().st_size\n                        try:\n                            async with aiofiles.open(file_path, encoding=\"utf-8\") as f:\n                                total_lines += sum(1 for _ in await f.readlines())\n                        except UnicodeDecodeError:\n                            binary_files += 1\n\n                statistics = {\n                    \"total_files\": total_files,\n                    \"total_size_bytes\": total_size,\n                    \"total_size_kb\": round(total_size / 1024, 2),\n                    \"total_size_mb\": round(total_size / (1024 * 1024), 2),\n                    \"total_lines\": total_lines,\n                    \"binary_files\": binary_files,\n                    \"directories\": directories,\n                }\n                result = [Data(data=statistics)]\n                self.status = result\n                return result\n        except git.GitError as e:\n            error_result = [Data(data={\"error\": f\"Error calculating statistics: {e!s}\"})]\n            self.status = error_result\n            return error_result\n\n    async def get_directory_structure(self) -> Message:\n        try:\n            async with self.temp_git_repo() as temp_dir:\n                tree = [\"Directory structure:\"]\n                for root, _dirs, files in os.walk(temp_dir):\n                    level = root.replace(temp_dir, \"\").count(os.sep)\n                    indent = \"    \" * level\n                    if level == 0:\n                        tree.append(f\"└── {Path(root).name}\")\n                    else:\n                        tree.append(f\"{indent}├── {Path(root).name}\")\n                    subindent = \"    \" * (level + 1)\n                    tree.extend(f\"{subindent}├── {f}\" for f in files)\n                directory_structure = \"\\n\".join(tree)\n                self.status = directory_structure\n                return Message(text=directory_structure)\n        except git.GitError as e:\n            error_message = f\"Error getting directory structure: {e!s}\"\n            self.status = error_message\n            return Message(text=error_message)\n\n    async def get_files_content(self) -> list[Data]:\n        try:\n            async with self.temp_git_repo() as temp_dir:\n                content_list = []\n                for root, _, files in os.walk(temp_dir):\n                    for file in files:\n                        file_path = Path(root) / file\n                        relative_path = file_path.relative_to(temp_dir)\n                        file_size = file_path.stat().st_size\n                        try:\n                            async with aiofiles.open(file_path, encoding=\"utf-8\") as f:\n                                file_content = await f.read()\n                        except UnicodeDecodeError:\n                            file_content = \"[BINARY FILE]\"\n                        content_list.append(\n                            Data(data={\"path\": str(relative_path), \"size\": file_size, \"content\": file_content})\n                        )\n                self.status = content_list\n                return content_list\n        except git.GitError as e:\n            error_result = [Data(data={\"error\": f\"Error getting files content: {e!s}\"})]\n            self.status = error_result\n            return error_result\n\n    async def get_text_based_file_contents(self) -> Message:\n        try:\n            async with self.temp_git_repo() as temp_dir:\n                content_list = [\"(Files content cropped to 300k characters, download full ingest to see more)\"]\n                total_chars = 0\n                char_limit = 300000\n\n                for root, _, files in os.walk(temp_dir):\n                    for file in files:\n                        file_path = Path(root) / file\n                        relative_path = file_path.relative_to(temp_dir)\n                        content_list.extend([\"=\" * 50, f\"File: /{relative_path}\", \"=\" * 50])\n\n                        try:\n                            async with aiofiles.open(file_path, encoding=\"utf-8\") as f:\n                                file_content = await f.read()\n                                if total_chars + len(file_content) > char_limit:\n                                    remaining_chars = char_limit - total_chars\n                                    file_content = file_content[:remaining_chars] + \"\\n... (content truncated)\"\n                                content_list.append(file_content)\n                                total_chars += len(file_content)\n                        except UnicodeDecodeError:\n                            content_list.append(\"[BINARY FILE]\")\n\n                        content_list.append(\"\")\n\n                        if total_chars >= char_limit:\n                            break\n\n                text_content = \"\\n\".join(content_list)\n                self.status = text_content\n                return Message(text=text_content)\n        except git.GitError as e:\n            error_message = f\"Error getting text-based file contents: {e!s}\"\n            self.status = error_message\n            return Message(text=error_message)\n"
              },
              "repository_url": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Repository URL",
                "dynamic": false,
                "info": "URL of the Git repository (e.g., https://github.com/username/repo)",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "repository_url",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "files_content",
          "showNode": true,
          "type": "GitExtractorComponent"
        },
        "dragging": false,
        "id": "GitExtractorComponent-PmJhm",
        "measured": {
          "height": 219,
          "width": 320
        },
        "position": {
          "x": -322.3868788512631,
          "y": -127.93685628511645
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "MetadataTagger-XqRJY",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Adds a custom key-value pair to the metadata of each document in a list.",
            "display_name": "Metadata Tagger",
            "documentation": "",
            "edited": true,
            "field_order": [
              "input_documents",
              "tag_name",
              "tag_value"
            ],
            "frozen": false,
            "icon": "tag",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Tagged Documents",
                "group_outputs": false,
                "hidden": null,
                "method": "tag_documents",
                "name": "tagged_documents",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import List\r\nfrom langflow.custom import Component\r\nfrom langflow.io import HandleInput, MessageTextInput, Output\r\nfrom langflow.schema import Data, Message\r\n\r\nclass MetadataTagger(Component):\r\n    display_name = \"Metadata Tagger\"\r\n    description = \"Adds a custom key-value pair to the metadata of each document in a list.\"\r\n    icon = \"tag\"\r\n    name = \"MetadataTagger\"\r\n\r\n    inputs = [\r\n        HandleInput(\r\n            name=\"input_documents\",\r\n            display_name=\"Input Data\",\r\n            info=\"The documents to tag. Can be a list of Data or a Message containing a list of Data.\",\r\n            is_list=True,\r\n            # --- CAMBIO IMPORTANTE: Aceptamos también objetos Message ---\r\n            input_types=[\"Data\", \"Message\"],\r\n            required=True,\r\n        ),\r\n        MessageTextInput(\r\n            name=\"tag_name\",\r\n            display_name=\"Tag Name\",\r\n            info=\"The name of the metadata field to add (e.g., 'origin').\",\r\n            value=\"origin\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"tag_value\",\r\n            display_name=\"Tag Value\",\r\n            info=\"The value to assign to the tag (e.g., 'PDF Document').\",\r\n            value=\"Unknown\",\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(\r\n            display_name=\"Tagged Documents\",\r\n            name=\"tagged_documents\",\r\n            method=\"tag_documents\"\r\n        ),\r\n    ]\r\n\r\n    def tag_documents(self) -> List[Data]:\r\n        \"\"\"\r\n        Extracts documents from the input and adds the specified tag to their metadata.\r\n        \"\"\"\r\n        raw_input = self.input_documents or []\r\n        documents: List[Data] = []\r\n        \r\n        # --- CAMBIO IMPORTANTE: Lógica para \"desenvolver\" la entrada ---\r\n        # Si la entrada es un Message, extraemos su contenido.\r\n        if isinstance(raw_input, Message):\r\n            content = raw_input.content\r\n            if isinstance(content, list):\r\n                documents = content\r\n            elif isinstance(content, Data):\r\n                documents = [content]\r\n        # Si ya es una lista (de Data), la usamos directamente.\r\n        elif isinstance(raw_input, list):\r\n            documents = raw_input\r\n        # Si es un solo objeto Data, lo metemos en una lista.\r\n        elif isinstance(raw_input, Data):\r\n            documents = [raw_input]\r\n\r\n        tag_name = self.tag_name\r\n        tag_value = self.tag_value\r\n        \r\n        if not documents:\r\n            self.status = \"No valid documents found in the input to tag.\"\r\n            return []\r\n\r\n        tagged_docs = []\r\n        for doc in documents:\r\n            if isinstance(doc, Data):\r\n                if not hasattr(doc, 'metadata') or not isinstance(doc.metadata, dict):\r\n                    doc.metadata = {}\r\n                \r\n                doc.metadata[tag_name] = tag_value\r\n                tagged_docs.append(doc)\r\n\r\n        self.status = f\"Successfully tagged {len(tagged_docs)} documents with '{tag_name}: {tag_value}'.\"\r\n        return tagged_docs"
              },
              "input_documents": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Input Data",
                "dynamic": false,
                "info": "The documents to tag. Can be a list of Data or a Message containing a list of Data.",
                "input_types": [
                  "Data",
                  "Message"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "input_documents",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "tag_name": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Tag Name",
                "dynamic": false,
                "info": "The name of the metadata field to add (e.g., 'origin').",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "tag_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "origin"
              },
              "tag_value": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Tag Value",
                "dynamic": false,
                "info": "The value to assign to the tag (e.g., 'PDF Document').",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "tag_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "GitHub Repository"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "MetadataTagger"
        },
        "dragging": false,
        "id": "MetadataTagger-XqRJY",
        "measured": {
          "height": 345,
          "width": 320
        },
        "position": {
          "x": 66.72295824131362,
          "y": -106.69739925207556
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "CustomComponent-F1TVs",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Restructures nested Data objects from loaders into a flat format for splitters.",
            "display_name": "Document Normalizer",
            "documentation": "",
            "edited": true,
            "field_order": [
              "input_documents"
            ],
            "frozen": false,
            "icon": "transform",
            "legacy": false,
            "lf_version": "1.5.1",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Normalized Documents",
                "group_outputs": false,
                "hidden": null,
                "method": "normalize_documents",
                "name": "normalized_documents",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import List\nfrom langflow.custom import Component\nfrom langflow.io import HandleInput, Output\nfrom langflow.schema import Data\n\nclass DocumentNormalizer(Component):\n    display_name = \"Document Normalizer\"\n    description = \"Restructures nested Data objects from loaders into a flat format for splitters.\"\n    icon = \"transform\"\n    name = \"DocumentNormalizer\"\n\n    inputs = [\n        HandleInput(\n            name=\"input_documents\",\n            display_name=\"Input Documents\",\n            info=\"The list of nested documents from a loader (e.g., GitExtractor).\",\n            is_list=True,\n            input_types=[\"Data\"],\n            required=True,\n        )\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Normalized Documents\",\n            name=\"normalized_documents\",\n            method=\"normalize_documents\"\n        ),\n    ]\n\n    def normalize_documents(self) -> List[Data]:\n        \"\"\"\n        Takes nested Data objects and flattens them into the standard format\n        that splitters and other components expect.\n        \"\"\"\n        nested_docs = self.input_documents or []\n        normalized_docs = []\n\n        if not nested_docs:\n            self.status = \"No documents to normalize.\"\n            return []\n\n        for doc in nested_docs:\n            # Nos aseguramos de que el objeto tenga la estructura anidada que esperamos\n            if not isinstance(doc, Data) or not hasattr(doc, 'data') or not doc.data:\n                continue\n\n            # Extraemos la información de la sub-caja '.data'\n            inner_data = doc.data\n            content = inner_data.get(\"content\", \"\")\n            path = inner_data.get(\"path\", \"Unknown Path\")\n            \n            # Extraemos los metadatos de la sub-sub-caja '.data['metadata']'\n            inner_metadata = inner_data.get(\"metadata\", {})\n            origin = inner_metadata.get(\"origin\", \"Unknown Origin\")\n            \n            # Creamos un nuevo objeto Data con la estructura \"plana\" y correcta\n            new_doc = Data(\n                text=content,\n                metadata={\n                    \"source\": path,  # Usamos 'path' como la 'source'\n                    \"origin\": origin   # Propagamos el 'origin' que agregamos antes\n                }\n            )\n            normalized_docs.append(new_doc)\n            \n        self.status = f\"Successfully normalized {len(normalized_docs)} documents.\"\n        return normalized_docs\n"
              },
              "input_documents": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Input Documents",
                "dynamic": false,
                "info": "The list of nested documents from a loader (e.g., GitExtractor).",
                "input_types": [
                  "Data"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "input_documents",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "DocumentNormalizer"
        },
        "dragging": false,
        "id": "CustomComponent-F1TVs",
        "measured": {
          "height": 181,
          "width": 320
        },
        "position": {
          "x": 427.0579630245183,
          "y": 39.42433807176252
        },
        "selected": false,
        "type": "genericNode"
      }
    ],
    "viewport": {
      "x": 340.0165506640433,
      "y": 213.5842336295242,
      "zoom": 0.8375544055210092
    }
  },
  "description": "Navigate the Linguistic Landscape, Discover Opportunities.",
  "endpoint_name": "ingest_github_flow",
  "id": "91d343c0-8632-418c-9776-023714d12054",
  "is_component": false,
  "last_tested_version": "1.5.1",
  "name": "Ingest Github Flow",
  "tags": []
}