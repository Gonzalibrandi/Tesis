import json
import os
from typing import Any, Dict, List

import numpy as np  # Necesario para calcular media y desv. estándar
import openai
import pandas as pd
from dotenv import load_dotenv

# Cargar variables de entorno (como OPENAI_API_KEY si está en un .env)
load_dotenv()

# --- CONFIGURACIÓN ---
BENCHMARK_FILE = "c:/Users/gonza/OneDrive/Escritorio/Tesis/Codigo/WebLovable/Benchmark/1er benchmark/rag_benchmark_questions_en.csv"
STATS_FILE = "c:/Users/gonza/OneDrive/Escritorio/Tesis/Codigo/WebLovable/Benchmark/1er benchmark/benchmark_statistics.csv" # El CSV final con 27 filas
JUDGE_MODEL = "gpt-4o-mini"
N_EVAL_RUNS = 10 # Número de veces que se evaluará CADA pregunta

# --- PROMPT PARA EL LLM JUEZ (LA RÚBRICA DE EVALUACIÓN) ---
# (El mismo prompt semántico que ya definimos)
JUDGE_PROMPT_TEMPLATE = """
You are a meticulous and impartial AI System Evaluator, acting as an expert judge. Your sole task is to assess the performance of a Retrieval-Augmented Generation (RAG) system by **semantically** comparing its actual output against a "golden" expected output.

You will be provided with the following information for each test case:
- **Question:** The original user query.
- **Expected Answer:** The ideal, correct answer, representing the key information that should be conveyed.
- **Actual Answer:** The answer generated by the RAG system being tested.
- **Expected Source Attribution:** The ideal percentage breakdown of sources.
- **Actual Source Attribution:** The percentage breakdown produced by the RAG system.

Based on this information, you MUST evaluate the system's performance on the following four metrics, providing a score from 1 (very poor) to 10 (excellent) for each. **Focus on the underlying meaning, not just literal string matching.**

**EVALUATION METRICS:**

1.  **Content Accuracy (Score 1-10):**
    - **Question:** How factually correct and **semantically equivalent** is the **Actual Answer** compared to the **Expected Answer**? Does it convey the same core meaning, even if phrased differently?
    - **10:** The answer is perfectly correct and captures the full meaning.
    - **1:** The answer is completely incorrect or misinterprets the core meaning.

2.  **Completeness (Score 1-10):**
    - **Question:** Does the **Actual Answer** include all the **critical concepts and key pieces of information** present in the **Expected Answer**?
    - **10:** All essential concepts are covered.
    - **1:** Critical concepts are missing, making the answer incomplete.

3.  **Source Attribution Accuracy (Score 1-10):**
    - **Question:** Does the **Actual Source Attribution** correctly reflect the **true origin of the information** used in the answer, consistent with the **Expected Source Attribution**? Prioritize identifying the correct primary source over exact percentage matching.
    - **10:** The primary source is correctly identified, and the percentages reasonably reflect the contribution.
    - **1:** The primary source is wrong, or the attribution is misleading.

4.  **Conciseness (Score 1-10):**
    - **Question:** Is the **Actual Answer** direct and to the point, without unnecessary verbosity or irrelevant information?
    - **10:** The answer is concise and well-phrased.
    - **1:** The answer is overly verbose or contains significant fluff.

**OUTPUT FORMAT (CRITICAL):**
You **MUST** provide your evaluation in a strict JSON format. **DO NOT** include any other text outside of the JSON object.

{{
  "metrics": {{
    "content_accuracy": {{ "score": <Float 1-10>, "reasoning": "Brief justification focusing on semantic meaning." }},
    "completeness": {{ "score": <Float 1-10>, "reasoning": "Brief justification listing any missing concepts." }},
    "source_attribution_accuracy": {{ "score": <Float 1-10>, "reasoning": "Brief justification focusing on the correctness of the primary source." }},
    "conciseness": {{ "score": <Float 1-10>, "reasoning": "Brief justification on clarity and brevity." }}
  }}
}}

**--- START OF TEST CASE ---**

**Question:**
{question}

**Expected Answer:**
{expected_answer}

**Actual Answer:**
{actual_answer}

**Expected Source Attribution:**
- PDF: {expected_pdf_perc}%
- GitHub: {expected_github_perc}%
- Web: {expected_web_perc}%

**Actual Source Attribution:**
- PDF: {actual_pdf_perc}%
- GitHub: {actual_github_perc}%
- Web: {actual_web_perc}%

**--- END OF TEST CASE ---**

**JSON Evaluation:**
"""

def evaluate_row(row: pd.Series, client: openai.OpenAI) -> Dict[str, Any]:
    """Construye el prompt para una fila y llama al LLM Juez."""
    prompt = JUDGE_PROMPT_TEMPLATE.format(
        question=row['Question'],
        expected_answer=row['Expected Answer'],
        actual_answer=row['Final Answer'],
        expected_pdf_perc=row['PDF'],
        expected_github_perc=row['GitHub'],
        expected_web_perc=row['Web'],
        actual_pdf_perc=row['PDF.1'],
        actual_github_perc=row['GitHub.1'],
        actual_web_perc=row['Web.1']
    )
    
    try:
        response = client.chat.completions.create(
            model=JUDGE_MODEL,
            messages=[{"role": "user", "content": prompt}],
            # --- CAMBIO CLAVE ---
            # Introducimos aleatoriedad para medir la estabilidad del Juez
            temperature=0.0, 
            response_format={"type": "json_object"}
        )
        json_response = json.loads(response.choices[0].message.content)
        
        # Aplanamos solo los scores para el análisis estadístico
        metrics = json_response.get('metrics', {})
        return {
            'accuracy_score': metrics.get('content_accuracy', {}).get('score'),
            'completeness_score': metrics.get('completeness', {}).get('score'),
            'attribution_score': metrics.get('source_attribution_accuracy', {}).get('score'),
            'conciseness_score': metrics.get('conciseness', {}).get('score'),
        }
    except Exception as e:
        print(f"Error al evaluar la pregunta '{row['Question']}': {e}")
        return {"error": str(e)}

def main():
    """Función principal para leer el benchmark, evaluarlo 10 veces por fila y calcular estadísticas."""
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("La variable de entorno OPENAI_API_KEY no está configurada.")
        
    client = openai.OpenAI()
    
    print(f"Leyendo el archivo de benchmark: {BENCHMARK_FILE}...")
    try:
        df = pd.read_csv(BENCHMARK_FILE, sep=';')
    except FileNotFoundError:
        print(f"❌ Error: No se encontró el archivo '{BENCHMARK_FILE}'.")
        return

    # Esta lista contendrá el resultado final de 27 filas
    statistics_results = []
    total_calls = len(df) * N_EVAL_RUNS
    call_count = 0

    print(f"Iniciando evaluación de {len(df)} casos de prueba, {N_EVAL_RUNS} veces cada uno (Total: {total_calls} llamadas a la API)...")
    
    for index, row in df.iterrows():
        print(f"\nProcesando Pregunta #{index + 1}/{len(df)}: '{row['Question'][:50]}...'")
        
        # Esta lista guardará los 10 scores para esta pregunta
        run_scores = {
            'accuracy_score': [],
            'completeness_score': [],
            'attribution_score': [],
            'conciseness_score': [],
        }
        
        # --- BUCLE INTERNO (10 ejecuciones) ---
        for i in range(N_EVAL_RUNS):
            call_count += 1
            print(f"  -> Ejecución {i+1}/{N_EVAL_RUNS} (Llamada {call_count}/{total_calls})")
            evaluation = evaluate_row(row, client)
            print(f"     Resultado del Juez: {evaluation}")
            
            if "error" not in evaluation:
                for key in run_scores.keys():
                    run_scores[key].append(evaluation.get(key))

        # --- CÁLCULO ESTADÍSTICO ---
        # Filtramos los 'None' que pudieron ocurrir por errores de parseo
        def safe_calculate(scores):
            valid_scores = [s for s in scores if s is not None]
            if not valid_scores:
                return (None, None)
            mean = np.mean(valid_scores)
            std_dev = np.std(valid_scores)
            return (mean, std_dev)

        acc_mean, acc_std = safe_calculate(run_scores['accuracy_score'])
        comp_mean, comp_std = safe_calculate(run_scores['completeness_score'])
        attr_mean, attr_std = safe_calculate(run_scores['attribution_score'])
        conc_mean, conc_std = safe_calculate(run_scores['conciseness_score'])

        # Guardamos la fila de estadísticas para esta pregunta
        statistics_results.append({
            'Question_ID': index,
            'Question': row['Question'],
            'Accuracy (Mean)': acc_mean, 'Accuracy (StdDev)': acc_std,
            'Completeness (Mean)': comp_mean, 'Completeness (StdDev)': comp_std,
            'Attribution (Mean)': attr_mean, 'Attribution (StdDev)': attr_std,
            'Conciseness (Mean)': conc_mean, 'Conciseness (StdDev)': conc_std,
        })

    print("\nEvaluación completada. Procesando resultados...")
    
    # Creamos el DataFrame final con las estadísticas
    stats_df = pd.DataFrame(statistics_results)
    
    # Guardamos el resultado final en un nuevo archivo CSV
    stats_df.to_csv(STATS_FILE, index=False, encoding='utf-8-sig', float_format='%.2f')
    
    print(f"✅ ¡Benchmark evaluado! Los resultados (media y desviación estándar) se han guardado en '{STATS_FILE}'.")
    
    print("\n--- Resumen de Estadísticas (Media y Desv. Estándar por Pregunta) ---")
    print(stats_df.to_string(float_format='%.2f'))

if __name__ == "__main__":
    main()